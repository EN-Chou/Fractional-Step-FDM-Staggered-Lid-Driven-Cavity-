{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a65886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da458150",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43e2973b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 82, 82])\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(1000, 6724)\n",
    "y=torch.randn(1000, 6724)\n",
    "\n",
    "x= torch.nn.Sequential(torch.nn.Unflatten(1, (1,82,82)))(x)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1fc6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x.to(device)\n",
    "y=y.to(device)\n",
    "loader=DataLoader(TensorDataset(x, y), batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76908c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=18252, out_features=1000, bias=True)\n",
      "  (fc2): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (fc3): Linear(in_features=1000, out_features=6724, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self, channel_1, channel_2, kernel_dim):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, channel_1, kernel_dim)\n",
    "        self.conv2 = nn.Conv2d(channel_1, channel_2, kernel_dim)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(78*78*3, 1000)  # 78*78 from image dimension\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 6724)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = ConvNet(channel_1=3, channel_2=3, kernel_dim=3).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8334cc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6724])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 82, 82)\n",
    "input=input.to(device)\n",
    "out = model(input)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "808cdd04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 3, 3)\n",
    "print(input.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "704b38b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n",
      "torch.Size([1, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 4)\n",
    "print(input.size())\n",
    "input= torch.nn.Sequential(torch.nn.Unflatten(1, ( 1,2,2)))(input)\n",
    "print(input.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5f1ee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6e3334b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs    Loss\n",
      "Epochs:  1 ; Loss:  0.9989970326423645\n",
      "Epochs:  2 ; Loss:  0.9984937906265259\n",
      "Epochs:  3 ; Loss:  0.9979026913642883\n",
      "Epochs:  4 ; Loss:  0.9969338774681091\n",
      "Epochs:  5 ; Loss:  0.995715856552124\n",
      "Epochs:  6 ; Loss:  0.9943270087242126\n",
      "Epochs:  7 ; Loss:  0.9927055835723877\n",
      "Epochs:  8 ; Loss:  0.9909926652908325\n",
      "Epochs:  9 ; Loss:  0.9891831874847412\n",
      "Epochs:  10 ; Loss:  0.9872633218765259\n",
      "Epochs:  11 ; Loss:  0.9851192235946655\n",
      "Epochs:  12 ; Loss:  0.9826411604881287\n",
      "Epochs:  13 ; Loss:  0.9798600077629089\n",
      "Epochs:  14 ; Loss:  0.9769915342330933\n",
      "Epochs:  15 ; Loss:  0.973777174949646\n",
      "Epochs:  16 ; Loss:  0.9705660343170166\n",
      "Epochs:  17 ; Loss:  0.9670158624649048\n",
      "Epochs:  18 ; Loss:  0.9633798599243164\n",
      "Epochs:  19 ; Loss:  0.959646463394165\n",
      "Epochs:  20 ; Loss:  0.9557541608810425\n",
      "Epochs:  21 ; Loss:  0.9517802596092224\n",
      "Epochs:  22 ; Loss:  0.9477421045303345\n",
      "Epochs:  23 ; Loss:  0.9442567229270935\n",
      "Epochs:  24 ; Loss:  0.9402888417243958\n",
      "Epochs:  25 ; Loss:  0.9363867044448853\n",
      "Epochs:  26 ; Loss:  0.9323635101318359\n",
      "Epochs:  27 ; Loss:  0.9288488626480103\n",
      "Epochs:  28 ; Loss:  0.9249631762504578\n",
      "Epochs:  29 ; Loss:  0.921110987663269\n",
      "Epochs:  30 ; Loss:  0.9177314043045044\n",
      "Epochs:  31 ; Loss:  0.9142295718193054\n",
      "Epochs:  32 ; Loss:  0.9104366302490234\n",
      "Epochs:  33 ; Loss:  0.907143235206604\n",
      "Epochs:  34 ; Loss:  0.9040278792381287\n",
      "Epochs:  35 ; Loss:  0.9005253911018372\n",
      "Epochs:  36 ; Loss:  0.8969719409942627\n",
      "Epochs:  37 ; Loss:  0.8927214741706848\n",
      "Epochs:  38 ; Loss:  0.8893242478370667\n",
      "Epochs:  39 ; Loss:  0.8862435817718506\n",
      "Epochs:  40 ; Loss:  0.8826386332511902\n",
      "Epochs:  41 ; Loss:  0.8794426321983337\n",
      "Epochs:  42 ; Loss:  0.8768944144248962\n",
      "Epochs:  43 ; Loss:  0.8745318651199341\n",
      "Epochs:  44 ; Loss:  0.8709889650344849\n",
      "Epochs:  45 ; Loss:  0.8675528764724731\n",
      "Epochs:  46 ; Loss:  0.8657402992248535\n",
      "Epochs:  47 ; Loss:  0.863330602645874\n",
      "Epochs:  48 ; Loss:  0.8601917028427124\n",
      "Epochs:  49 ; Loss:  0.8567408323287964\n",
      "Epochs:  50 ; Loss:  0.8538927435874939\n",
      "Epochs:  51 ; Loss:  0.8514949083328247\n",
      "Epochs:  52 ; Loss:  0.8495460748672485\n",
      "Epochs:  53 ; Loss:  0.8460699915885925\n",
      "Epochs:  54 ; Loss:  0.8430947661399841\n",
      "Epochs:  55 ; Loss:  0.8407089710235596\n",
      "Epochs:  56 ; Loss:  0.8378744721412659\n",
      "Epochs:  57 ; Loss:  0.8360322117805481\n",
      "Epochs:  58 ; Loss:  0.8331992626190186\n",
      "Epochs:  59 ; Loss:  0.830251932144165\n",
      "Epochs:  60 ; Loss:  0.8282750844955444\n",
      "Epochs:  61 ; Loss:  0.8248822689056396\n",
      "Epochs:  62 ; Loss:  0.8225200772285461\n",
      "Epochs:  63 ; Loss:  0.8202670216560364\n",
      "Epochs:  64 ; Loss:  0.8178133368492126\n",
      "Epochs:  65 ; Loss:  0.8149957656860352\n",
      "Epochs:  66 ; Loss:  0.8128343224525452\n",
      "Epochs:  67 ; Loss:  0.8112432956695557\n",
      "Epochs:  68 ; Loss:  0.8093753457069397\n",
      "Epochs:  69 ; Loss:  0.8069913387298584\n",
      "Epochs:  70 ; Loss:  0.8041508793830872\n",
      "Epochs:  71 ; Loss:  0.8014382123947144\n",
      "Epochs:  72 ; Loss:  0.7995802164077759\n",
      "Epochs:  73 ; Loss:  0.7988654375076294\n",
      "Epochs:  74 ; Loss:  0.7966029047966003\n",
      "Epochs:  75 ; Loss:  0.7928863763809204\n",
      "Epochs:  76 ; Loss:  0.7900190949440002\n",
      "Epochs:  77 ; Loss:  0.7874594330787659\n",
      "Epochs:  78 ; Loss:  0.78560870885849\n",
      "Epochs:  79 ; Loss:  0.7842127680778503\n",
      "Epochs:  80 ; Loss:  0.7822930812835693\n",
      "Epochs:  81 ; Loss:  0.7801754474639893\n",
      "Epochs:  82 ; Loss:  0.7778671383857727\n",
      "Epochs:  83 ; Loss:  0.7751699090003967\n",
      "Epochs:  84 ; Loss:  0.7733514308929443\n",
      "Epochs:  85 ; Loss:  0.7726570963859558\n",
      "Epochs:  86 ; Loss:  0.7711683511734009\n",
      "Epochs:  87 ; Loss:  0.7687801122665405\n",
      "Epochs:  88 ; Loss:  0.7663202881813049\n",
      "Epochs:  89 ; Loss:  0.7638916373252869\n",
      "Epochs:  90 ; Loss:  0.7626261115074158\n",
      "Epochs:  91 ; Loss:  0.7605792880058289\n",
      "Epochs:  92 ; Loss:  0.7578270435333252\n",
      "Epochs:  93 ; Loss:  0.7558492422103882\n",
      "Epochs:  94 ; Loss:  0.7553294897079468\n",
      "Epochs:  95 ; Loss:  0.7552394866943359\n",
      "Epochs:  96 ; Loss:  0.7529513239860535\n",
      "Epochs:  97 ; Loss:  0.7507636547088623\n",
      "Epochs:  98 ; Loss:  0.7484520077705383\n",
      "Epochs:  99 ; Loss:  0.7451278567314148\n",
      "Epochs:  100 ; Loss:  0.7436054348945618\n",
      "Epochs:  101 ; Loss:  0.7430563569068909\n",
      "Epochs:  102 ; Loss:  0.7418156266212463\n",
      "Epochs:  103 ; Loss:  0.7399374842643738\n",
      "Epochs:  104 ; Loss:  0.7383375763893127\n",
      "Epochs:  105 ; Loss:  0.7362334132194519\n",
      "Epochs:  106 ; Loss:  0.7337502241134644\n",
      "Epochs:  107 ; Loss:  0.7313169240951538\n",
      "Epochs:  108 ; Loss:  0.7293148636817932\n",
      "Epochs:  109 ; Loss:  0.7282718420028687\n",
      "Epochs:  110 ; Loss:  0.7269826531410217\n",
      "Epochs:  111 ; Loss:  0.7249991297721863\n",
      "Epochs:  112 ; Loss:  0.7232418060302734\n",
      "Epochs:  113 ; Loss:  0.7219477295875549\n",
      "Epochs:  114 ; Loss:  0.7208579182624817\n",
      "Epochs:  115 ; Loss:  0.7197590470314026\n",
      "Epochs:  116 ; Loss:  0.7185432314872742\n",
      "Epochs:  117 ; Loss:  0.7168357372283936\n",
      "Epochs:  118 ; Loss:  0.7149012684822083\n",
      "Epochs:  119 ; Loss:  0.7134612798690796\n",
      "Epochs:  120 ; Loss:  0.7119160890579224\n",
      "Epochs:  121 ; Loss:  0.7106042504310608\n",
      "Epochs:  122 ; Loss:  0.709132730960846\n",
      "Epochs:  123 ; Loss:  0.7065299153327942\n",
      "Epochs:  124 ; Loss:  0.7041870951652527\n",
      "Epochs:  125 ; Loss:  0.7021587491035461\n",
      "Epochs:  126 ; Loss:  0.7006329298019409\n",
      "Epochs:  127 ; Loss:  0.7001678347587585\n",
      "Epochs:  128 ; Loss:  0.6993803381919861\n",
      "Epochs:  129 ; Loss:  0.6973199248313904\n",
      "Epochs:  130 ; Loss:  0.694990336894989\n",
      "Epochs:  131 ; Loss:  0.6933962106704712\n",
      "Epochs:  132 ; Loss:  0.6925241351127625\n",
      "Epochs:  133 ; Loss:  0.6922871470451355\n",
      "Epochs:  134 ; Loss:  0.6917290687561035\n",
      "Epochs:  135 ; Loss:  0.6898254156112671\n",
      "Epochs:  136 ; Loss:  0.6878979206085205\n",
      "Epochs:  137 ; Loss:  0.6868983507156372\n",
      "Epochs:  138 ; Loss:  0.6861170530319214\n",
      "Epochs:  139 ; Loss:  0.6852198839187622\n",
      "Epochs:  140 ; Loss:  0.6838046908378601\n",
      "Epochs:  141 ; Loss:  0.6819682717323303\n",
      "Epochs:  142 ; Loss:  0.6808400750160217\n",
      "Epochs:  143 ; Loss:  0.6793838739395142\n",
      "Epochs:  144 ; Loss:  0.677054762840271\n",
      "Epochs:  145 ; Loss:  0.6743838787078857\n",
      "Epochs:  146 ; Loss:  0.672092616558075\n",
      "Epochs:  147 ; Loss:  0.670855700969696\n",
      "Epochs:  148 ; Loss:  0.6703229546546936\n",
      "Epochs:  149 ; Loss:  0.6696439385414124\n",
      "Epochs:  150 ; Loss:  0.6682596206665039\n",
      "Epochs:  151 ; Loss:  0.6665806770324707\n",
      "Epochs:  152 ; Loss:  0.6650001406669617\n",
      "Epochs:  153 ; Loss:  0.6634701490402222\n",
      "Epochs:  154 ; Loss:  0.6628064513206482\n",
      "Epochs:  155 ; Loss:  0.6635056734085083\n",
      "Epochs:  156 ; Loss:  0.663706362247467\n",
      "Epochs:  157 ; Loss:  0.6621164679527283\n",
      "Epochs:  158 ; Loss:  0.6594086289405823\n",
      "Epochs:  159 ; Loss:  0.656415581703186\n",
      "Epochs:  160 ; Loss:  0.6549760103225708\n",
      "Epochs:  161 ; Loss:  0.6546721458435059\n",
      "Epochs:  162 ; Loss:  0.6536477208137512\n",
      "Epochs:  163 ; Loss:  0.6526304483413696\n",
      "Epochs:  164 ; Loss:  0.6519657969474792\n",
      "Epochs:  165 ; Loss:  0.6510440111160278\n",
      "Epochs:  166 ; Loss:  0.6497424244880676\n",
      "Epochs:  167 ; Loss:  0.6481900215148926\n",
      "Epochs:  168 ; Loss:  0.6474673748016357\n",
      "Epochs:  169 ; Loss:  0.6474006175994873\n",
      "Epochs:  170 ; Loss:  0.6461737155914307\n",
      "Epochs:  171 ; Loss:  0.6443281769752502\n",
      "Epochs:  172 ; Loss:  0.6424710750579834\n",
      "Epochs:  173 ; Loss:  0.6408316493034363\n",
      "Epochs:  174 ; Loss:  0.6400589346885681\n",
      "Epochs:  175 ; Loss:  0.6393657922744751\n",
      "Epochs:  176 ; Loss:  0.6383567452430725\n",
      "Epochs:  177 ; Loss:  0.6373988389968872\n",
      "Epochs:  178 ; Loss:  0.6362191438674927\n",
      "Epochs:  179 ; Loss:  0.6345673203468323\n",
      "Epochs:  180 ; Loss:  0.6328141689300537\n",
      "Epochs:  181 ; Loss:  0.6313049793243408\n",
      "Epochs:  182 ; Loss:  0.6299636363983154\n",
      "Epochs:  183 ; Loss:  0.6288878321647644\n",
      "Epochs:  184 ; Loss:  0.6279182434082031\n",
      "Epochs:  185 ; Loss:  0.6263551115989685\n",
      "Epochs:  186 ; Loss:  0.6246365904808044\n",
      "Epochs:  187 ; Loss:  0.6240617632865906\n",
      "Epochs:  188 ; Loss:  0.6244381070137024\n",
      "Epochs:  189 ; Loss:  0.6237821578979492\n",
      "Epochs:  190 ; Loss:  0.6216313242912292\n",
      "Epochs:  191 ; Loss:  0.6195858716964722\n",
      "Epochs:  192 ; Loss:  0.618331253528595\n",
      "Epochs:  193 ; Loss:  0.6176763772964478\n",
      "Epochs:  194 ; Loss:  0.6178650856018066\n",
      "Epochs:  195 ; Loss:  0.6182206869125366\n",
      "Epochs:  196 ; Loss:  0.6171019077301025\n",
      "Epochs:  197 ; Loss:  0.6150075793266296\n",
      "Epochs:  198 ; Loss:  0.6143582463264465\n",
      "Epochs:  199 ; Loss:  0.6146643161773682\n",
      "Epochs:  200 ; Loss:  0.6129046082496643\n",
      "Epochs:  201 ; Loss:  0.6109116077423096\n",
      "Epochs:  202 ; Loss:  0.6108763813972473\n",
      "Epochs:  203 ; Loss:  0.6109459400177002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:  204 ; Loss:  0.6108357310295105\n",
      "Epochs:  205 ; Loss:  0.6097609996795654\n",
      "Epochs:  206 ; Loss:  0.6069252490997314\n",
      "Epochs:  207 ; Loss:  0.6047159433364868\n",
      "Epochs:  208 ; Loss:  0.6040868163108826\n",
      "Epochs:  209 ; Loss:  0.6036326885223389\n",
      "Epochs:  210 ; Loss:  0.6028020977973938\n",
      "Epochs:  211 ; Loss:  0.6012420654296875\n",
      "Epochs:  212 ; Loss:  0.5994646549224854\n",
      "Epochs:  213 ; Loss:  0.5993980765342712\n",
      "Epochs:  214 ; Loss:  0.6007200479507446\n",
      "Epochs:  215 ; Loss:  0.6002305150032043\n",
      "Epochs:  216 ; Loss:  0.5975459218025208\n",
      "Epochs:  217 ; Loss:  0.5957473516464233\n",
      "Epochs:  218 ; Loss:  0.5949562191963196\n",
      "Epochs:  219 ; Loss:  0.5937219262123108\n",
      "Epochs:  220 ; Loss:  0.592682421207428\n",
      "Epochs:  221 ; Loss:  0.5916838645935059\n",
      "Epochs:  222 ; Loss:  0.5905284881591797\n",
      "Epochs:  223 ; Loss:  0.5895203948020935\n",
      "Epochs:  224 ; Loss:  0.5884223580360413\n",
      "Epochs:  225 ; Loss:  0.5873481631278992\n",
      "Epochs:  226 ; Loss:  0.5864571332931519\n",
      "Epochs:  227 ; Loss:  0.5855309963226318\n",
      "Epochs:  228 ; Loss:  0.5846549272537231\n",
      "Epochs:  229 ; Loss:  0.5844762325286865\n",
      "Epochs:  230 ; Loss:  0.5848334431648254\n",
      "Epochs:  231 ; Loss:  0.5844627022743225\n",
      "Epochs:  232 ; Loss:  0.5832198858261108\n",
      "Epochs:  233 ; Loss:  0.582472562789917\n",
      "Epochs:  234 ; Loss:  0.5816367268562317\n",
      "Epochs:  235 ; Loss:  0.5801894068717957\n",
      "Epochs:  236 ; Loss:  0.5799723863601685\n",
      "Epochs:  237 ; Loss:  0.5807680487632751\n",
      "Epochs:  238 ; Loss:  0.5798629522323608\n",
      "Epochs:  239 ; Loss:  0.5778066515922546\n",
      "Epochs:  240 ; Loss:  0.5759009122848511\n",
      "Epochs:  241 ; Loss:  0.5742411613464355\n",
      "Epochs:  242 ; Loss:  0.5740723609924316\n",
      "Epochs:  243 ; Loss:  0.5741232633590698\n",
      "Epochs:  244 ; Loss:  0.5730122923851013\n",
      "Epochs:  245 ; Loss:  0.5707710981369019\n",
      "Epochs:  246 ; Loss:  0.5684332847595215\n",
      "Epochs:  247 ; Loss:  0.567650556564331\n",
      "Epochs:  248 ; Loss:  0.5679981708526611\n",
      "Epochs:  249 ; Loss:  0.5680865049362183\n",
      "Epochs:  250 ; Loss:  0.5672430396080017\n",
      "Epochs:  251 ; Loss:  0.5654778480529785\n",
      "Epochs:  252 ; Loss:  0.5639617443084717\n",
      "Epochs:  253 ; Loss:  0.563818633556366\n",
      "Epochs:  254 ; Loss:  0.5640219449996948\n",
      "Epochs:  255 ; Loss:  0.5632261037826538\n",
      "Epochs:  256 ; Loss:  0.5619034171104431\n",
      "Epochs:  257 ; Loss:  0.5608406662940979\n",
      "Epochs:  258 ; Loss:  0.5599576830863953\n",
      "Epochs:  259 ; Loss:  0.5591226816177368\n",
      "Epochs:  260 ; Loss:  0.5582884550094604\n",
      "Epochs:  261 ; Loss:  0.557469367980957\n",
      "Epochs:  262 ; Loss:  0.5574725866317749\n",
      "Epochs:  263 ; Loss:  0.5579664707183838\n",
      "Epochs:  264 ; Loss:  0.5567875504493713\n",
      "Epochs:  265 ; Loss:  0.5542369484901428\n",
      "Epochs:  266 ; Loss:  0.5533552765846252\n",
      "Epochs:  267 ; Loss:  0.553425669670105\n",
      "Epochs:  268 ; Loss:  0.5520893335342407\n",
      "Epochs:  269 ; Loss:  0.5506748557090759\n",
      "Epochs:  270 ; Loss:  0.5503491759300232\n",
      "Epochs:  271 ; Loss:  0.550580620765686\n",
      "Epochs:  272 ; Loss:  0.5505900979042053\n",
      "Epochs:  273 ; Loss:  0.549947202205658\n",
      "Epochs:  274 ; Loss:  0.548966109752655\n",
      "Epochs:  275 ; Loss:  0.5479215979576111\n",
      "Epochs:  276 ; Loss:  0.5467481017112732\n",
      "Epochs:  277 ; Loss:  0.5466077327728271\n",
      "Epochs:  278 ; Loss:  0.547910749912262\n",
      "Epochs:  279 ; Loss:  0.5479591488838196\n",
      "Epochs:  280 ; Loss:  0.5459563732147217\n",
      "Epochs:  281 ; Loss:  0.5440739393234253\n",
      "Epochs:  282 ; Loss:  0.542357325553894\n",
      "Epochs:  283 ; Loss:  0.5417522192001343\n",
      "Epochs:  284 ; Loss:  0.5417733788490295\n",
      "Epochs:  285 ; Loss:  0.5399507284164429\n",
      "Epochs:  286 ; Loss:  0.5376641154289246\n",
      "Epochs:  287 ; Loss:  0.5364458560943604\n",
      "Epochs:  288 ; Loss:  0.5358738899230957\n",
      "Epochs:  289 ; Loss:  0.5354518294334412\n",
      "Epochs:  290 ; Loss:  0.5352010130882263\n",
      "Epochs:  291 ; Loss:  0.5352943539619446\n",
      "Epochs:  292 ; Loss:  0.535568118095398\n",
      "Epochs:  293 ; Loss:  0.5350674390792847\n",
      "Epochs:  294 ; Loss:  0.533336341381073\n",
      "Epochs:  295 ; Loss:  0.5318760275840759\n",
      "Epochs:  296 ; Loss:  0.532668948173523\n",
      "Epochs:  297 ; Loss:  0.5341477394104004\n",
      "Epochs:  298 ; Loss:  0.5335322022438049\n",
      "Epochs:  299 ; Loss:  0.5324185490608215\n",
      "Epochs:  300 ; Loss:  0.5309814810752869\n",
      "Epochs:  301 ; Loss:  0.5286582112312317\n",
      "Epochs:  302 ; Loss:  0.5274724364280701\n",
      "Epochs:  303 ; Loss:  0.5268818140029907\n",
      "Epochs:  304 ; Loss:  0.5265776515007019\n",
      "Epochs:  305 ; Loss:  0.5269671678543091\n",
      "Epochs:  306 ; Loss:  0.5262460112571716\n",
      "Epochs:  307 ; Loss:  0.5247187614440918\n",
      "Epochs:  308 ; Loss:  0.5242984890937805\n",
      "Epochs:  309 ; Loss:  0.5244308710098267\n",
      "Epochs:  310 ; Loss:  0.5233989953994751\n",
      "Epochs:  311 ; Loss:  0.5215669274330139\n",
      "Epochs:  312 ; Loss:  0.5208664536476135\n",
      "Epochs:  313 ; Loss:  0.5217084884643555\n",
      "Epochs:  314 ; Loss:  0.5222495198249817\n",
      "Epochs:  315 ; Loss:  0.5212291479110718\n",
      "Epochs:  316 ; Loss:  0.5198495984077454\n",
      "Epochs:  317 ; Loss:  0.518815815448761\n",
      "Epochs:  318 ; Loss:  0.5176764130592346\n",
      "Epochs:  319 ; Loss:  0.5166645646095276\n",
      "Epochs:  320 ; Loss:  0.5161545276641846\n",
      "Epochs:  321 ; Loss:  0.5160982608795166\n",
      "Epochs:  322 ; Loss:  0.5156203508377075\n",
      "Epochs:  323 ; Loss:  0.5144134759902954\n",
      "Epochs:  324 ; Loss:  0.5132747292518616\n",
      "Epochs:  325 ; Loss:  0.5126335620880127\n",
      "Epochs:  326 ; Loss:  0.5123186111450195\n",
      "Epochs:  327 ; Loss:  0.5119711756706238\n",
      "Epochs:  328 ; Loss:  0.5111290812492371\n",
      "Epochs:  329 ; Loss:  0.5102593302726746\n",
      "Epochs:  330 ; Loss:  0.5102152228355408\n",
      "Epochs:  331 ; Loss:  0.5105878114700317\n",
      "Epochs:  332 ; Loss:  0.5104712247848511\n",
      "Epochs:  333 ; Loss:  0.5094937682151794\n",
      "Epochs:  334 ; Loss:  0.5078058838844299\n",
      "Epochs:  335 ; Loss:  0.5061452984809875\n",
      "Epochs:  336 ; Loss:  0.5057028532028198\n",
      "Epochs:  337 ; Loss:  0.5056450366973877\n",
      "Epochs:  338 ; Loss:  0.5045958161354065\n",
      "Epochs:  339 ; Loss:  0.5031388998031616\n",
      "Epochs:  340 ; Loss:  0.5022203922271729\n",
      "Epochs:  341 ; Loss:  0.5017107129096985\n",
      "Epochs:  342 ; Loss:  0.5017179846763611\n",
      "Epochs:  343 ; Loss:  0.5024940967559814\n",
      "Epochs:  344 ; Loss:  0.5031236410140991\n",
      "Epochs:  345 ; Loss:  0.5020734071731567\n",
      "Epochs:  346 ; Loss:  0.4998387098312378\n",
      "Epochs:  347 ; Loss:  0.4988183081150055\n",
      "Epochs:  348 ; Loss:  0.49892231822013855\n",
      "Epochs:  349 ; Loss:  0.49840444326400757\n",
      "Epochs:  350 ; Loss:  0.4979929029941559\n",
      "Epochs:  351 ; Loss:  0.4977205991744995\n",
      "Epochs:  352 ; Loss:  0.49678370356559753\n",
      "Epochs:  353 ; Loss:  0.49621883034706116\n",
      "Epochs:  354 ; Loss:  0.4964172840118408\n",
      "Epochs:  355 ; Loss:  0.4957011044025421\n",
      "Epochs:  356 ; Loss:  0.4937984049320221\n",
      "Epochs:  357 ; Loss:  0.4925641119480133\n",
      "Epochs:  358 ; Loss:  0.4919649660587311\n",
      "Epochs:  359 ; Loss:  0.49108853936195374\n",
      "Epochs:  360 ; Loss:  0.4902251660823822\n",
      "Epochs:  361 ; Loss:  0.489820271730423\n",
      "Epochs:  362 ; Loss:  0.4901934266090393\n",
      "Epochs:  363 ; Loss:  0.4911023676395416\n",
      "Epochs:  364 ; Loss:  0.4911364018917084\n",
      "Epochs:  365 ; Loss:  0.4895889163017273\n",
      "Epochs:  366 ; Loss:  0.48777854442596436\n",
      "Epochs:  367 ; Loss:  0.4868677258491516\n",
      "Epochs:  368 ; Loss:  0.486376017332077\n",
      "Epochs:  369 ; Loss:  0.4860824942588806\n",
      "Epochs:  370 ; Loss:  0.48595094680786133\n",
      "Epochs:  371 ; Loss:  0.48519977927207947\n",
      "Epochs:  372 ; Loss:  0.48406466841697693\n",
      "Epochs:  373 ; Loss:  0.4833788275718689\n",
      "Epochs:  374 ; Loss:  0.4830563962459564\n",
      "Epochs:  375 ; Loss:  0.48309430480003357\n",
      "Epochs:  376 ; Loss:  0.48394012451171875\n",
      "Epochs:  377 ; Loss:  0.4842236638069153\n",
      "Epochs:  378 ; Loss:  0.48224544525146484\n",
      "Epochs:  379 ; Loss:  0.48025205731391907\n",
      "Epochs:  380 ; Loss:  0.4809081256389618\n",
      "Epochs:  381 ; Loss:  0.48170414566993713\n",
      "Epochs:  382 ; Loss:  0.48012205958366394\n",
      "Epochs:  383 ; Loss:  0.4783645570278168\n",
      "Epochs:  384 ; Loss:  0.4778418242931366\n",
      "Epochs:  385 ; Loss:  0.47801673412323\n",
      "Epochs:  386 ; Loss:  0.4777112603187561\n",
      "Epochs:  387 ; Loss:  0.47646743059158325\n",
      "Epochs:  388 ; Loss:  0.4756165146827698\n",
      "Epochs:  389 ; Loss:  0.47529053688049316\n",
      "Epochs:  390 ; Loss:  0.4746350049972534\n",
      "Epochs:  391 ; Loss:  0.47429159283638\n",
      "Epochs:  392 ; Loss:  0.47428175806999207\n",
      "Epochs:  393 ; Loss:  0.47344881296157837\n",
      "Epochs:  394 ; Loss:  0.4722672700881958\n",
      "Epochs:  395 ; Loss:  0.47226592898368835\n",
      "Epochs:  396 ; Loss:  0.47281110286712646\n",
      "Epochs:  397 ; Loss:  0.4726921021938324\n",
      "Epochs:  398 ; Loss:  0.47191494703292847\n",
      "Epochs:  399 ; Loss:  0.47163712978363037\n",
      "Epochs:  400 ; Loss:  0.47256582975387573\n",
      "Epochs:  401 ; Loss:  0.4738558530807495\n",
      "Epochs:  402 ; Loss:  0.4724698066711426\n",
      "Epochs:  403 ; Loss:  0.46970677375793457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:  404 ; Loss:  0.4690741002559662\n",
      "Epochs:  405 ; Loss:  0.468241810798645\n",
      "Epochs:  406 ; Loss:  0.4673086404800415\n",
      "Epochs:  407 ; Loss:  0.4666769504547119\n",
      "Epochs:  408 ; Loss:  0.4651685059070587\n",
      "Epochs:  409 ; Loss:  0.46428096294403076\n",
      "Epochs:  410 ; Loss:  0.4640306532382965\n",
      "Epochs:  411 ; Loss:  0.46374839544296265\n",
      "Epochs:  412 ; Loss:  0.4631724953651428\n",
      "Epochs:  413 ; Loss:  0.46229252219200134\n",
      "Epochs:  414 ; Loss:  0.46147093176841736\n",
      "Epochs:  415 ; Loss:  0.4609399139881134\n",
      "Epochs:  416 ; Loss:  0.46101775765419006\n",
      "Epochs:  417 ; Loss:  0.46167564392089844\n",
      "Epochs:  418 ; Loss:  0.46182310581207275\n",
      "Epochs:  419 ; Loss:  0.46071669459342957\n",
      "Epochs:  420 ; Loss:  0.4594693183898926\n",
      "Epochs:  421 ; Loss:  0.4587554633617401\n",
      "Epochs:  422 ; Loss:  0.4578987956047058\n",
      "Epochs:  423 ; Loss:  0.4570467472076416\n",
      "Epochs:  424 ; Loss:  0.4564476013183594\n",
      "Epochs:  425 ; Loss:  0.456102192401886\n",
      "Epochs:  426 ; Loss:  0.45625340938568115\n",
      "Epochs:  427 ; Loss:  0.45643699169158936\n",
      "Epochs:  428 ; Loss:  0.4557148814201355\n",
      "Epochs:  429 ; Loss:  0.45464780926704407\n",
      "Epochs:  430 ; Loss:  0.45389625430107117\n",
      "Epochs:  431 ; Loss:  0.45315083861351013\n",
      "Epochs:  432 ; Loss:  0.45248353481292725\n",
      "Epochs:  433 ; Loss:  0.45235559344291687\n",
      "Epochs:  434 ; Loss:  0.45227542519569397\n",
      "Epochs:  435 ; Loss:  0.4517107307910919\n",
      "Epochs:  436 ; Loss:  0.4509124755859375\n",
      "Epochs:  437 ; Loss:  0.4500913619995117\n",
      "Epochs:  438 ; Loss:  0.44924700260162354\n",
      "Epochs:  439 ; Loss:  0.4486095905303955\n",
      "Epochs:  440 ; Loss:  0.4482855200767517\n",
      "Epochs:  441 ; Loss:  0.4482180178165436\n",
      "Epochs:  442 ; Loss:  0.44823911786079407\n",
      "Epochs:  443 ; Loss:  0.4480046033859253\n",
      "Epochs:  444 ; Loss:  0.44739043712615967\n",
      "Epochs:  445 ; Loss:  0.4470328092575073\n",
      "Epochs:  446 ; Loss:  0.44681689143180847\n",
      "Epochs:  447 ; Loss:  0.44579869508743286\n",
      "Epochs:  448 ; Loss:  0.44434216618537903\n",
      "Epochs:  449 ; Loss:  0.4438345730304718\n",
      "Epochs:  450 ; Loss:  0.44395753741264343\n",
      "Epochs:  451 ; Loss:  0.4438854455947876\n",
      "Epochs:  452 ; Loss:  0.4434414803981781\n",
      "Epochs:  453 ; Loss:  0.44256412982940674\n",
      "Epochs:  454 ; Loss:  0.44170573353767395\n",
      "Epochs:  455 ; Loss:  0.44119876623153687\n",
      "Epochs:  456 ; Loss:  0.4406887888908386\n",
      "Epochs:  457 ; Loss:  0.44025713205337524\n",
      "Epochs:  458 ; Loss:  0.44065138697624207\n",
      "Epochs:  459 ; Loss:  0.4413803517818451\n",
      "Epochs:  460 ; Loss:  0.44105812907218933\n",
      "Epochs:  461 ; Loss:  0.43986809253692627\n",
      "Epochs:  462 ; Loss:  0.43923500180244446\n",
      "Epochs:  463 ; Loss:  0.4393308460712433\n",
      "Epochs:  464 ; Loss:  0.43873539566993713\n",
      "Epochs:  465 ; Loss:  0.43710196018218994\n",
      "Epochs:  466 ; Loss:  0.43573257327079773\n",
      "Epochs:  467 ; Loss:  0.43508341908454895\n",
      "Epochs:  468 ; Loss:  0.4346599280834198\n",
      "Epochs:  469 ; Loss:  0.4339843988418579\n",
      "Epochs:  470 ; Loss:  0.43306800723075867\n",
      "Epochs:  471 ; Loss:  0.4323229193687439\n",
      "Epochs:  472 ; Loss:  0.4320053458213806\n",
      "Epochs:  473 ; Loss:  0.43175944685935974\n",
      "Epochs:  474 ; Loss:  0.4311026632785797\n",
      "Epochs:  475 ; Loss:  0.43055349588394165\n",
      "Epochs:  476 ; Loss:  0.4308672249317169\n",
      "Epochs:  477 ; Loss:  0.4314153790473938\n",
      "Epochs:  478 ; Loss:  0.4307955503463745\n",
      "Epochs:  479 ; Loss:  0.42960941791534424\n",
      "Epochs:  480 ; Loss:  0.4297420382499695\n",
      "Epochs:  481 ; Loss:  0.4310357868671417\n",
      "Epochs:  482 ; Loss:  0.4318312406539917\n",
      "Epochs:  483 ; Loss:  0.4316914975643158\n",
      "Epochs:  484 ; Loss:  0.4296760559082031\n",
      "Epochs:  485 ; Loss:  0.4265824556350708\n",
      "Epochs:  486 ; Loss:  0.42684420943260193\n",
      "Epochs:  487 ; Loss:  0.42814844846725464\n",
      "Epochs:  488 ; Loss:  0.426480770111084\n",
      "Epochs:  489 ; Loss:  0.42470794916152954\n",
      "Epochs:  490 ; Loss:  0.4238978326320648\n",
      "Epochs:  491 ; Loss:  0.424058198928833\n",
      "Epochs:  492 ; Loss:  0.42512568831443787\n",
      "Epochs:  493 ; Loss:  0.42508140206336975\n",
      "Epochs:  494 ; Loss:  0.4238975942134857\n",
      "Epochs:  495 ; Loss:  0.4234561324119568\n",
      "Epochs:  496 ; Loss:  0.42327696084976196\n",
      "Epochs:  497 ; Loss:  0.4225652813911438\n",
      "Epochs:  498 ; Loss:  0.42200562357902527\n",
      "Epochs:  499 ; Loss:  0.42157530784606934\n",
      "Epochs:  500 ; Loss:  0.42055732011795044\n",
      "Epochs:  501 ; Loss:  0.41997620463371277\n",
      "Epochs:  502 ; Loss:  0.41976118087768555\n",
      "Epochs:  503 ; Loss:  0.4191519618034363\n",
      "Epochs:  504 ; Loss:  0.4185347557067871\n",
      "Epochs:  505 ; Loss:  0.41795864701271057\n",
      "Epochs:  506 ; Loss:  0.4173526465892792\n",
      "Epochs:  507 ; Loss:  0.4168448746204376\n",
      "Epochs:  508 ; Loss:  0.4161408543586731\n",
      "Epochs:  509 ; Loss:  0.41552627086639404\n",
      "Epochs:  510 ; Loss:  0.4151802062988281\n",
      "Epochs:  511 ; Loss:  0.4144766330718994\n",
      "Epochs:  512 ; Loss:  0.4136030673980713\n",
      "Epochs:  513 ; Loss:  0.4134915769100189\n",
      "Epochs:  514 ; Loss:  0.41369059681892395\n",
      "Epochs:  515 ; Loss:  0.41298648715019226\n",
      "Epochs:  516 ; Loss:  0.41168564558029175\n",
      "Epochs:  517 ; Loss:  0.41094720363616943\n",
      "Epochs:  518 ; Loss:  0.41102108359336853\n",
      "Epochs:  519 ; Loss:  0.41089344024658203\n",
      "Epochs:  520 ; Loss:  0.4100699722766876\n",
      "Epochs:  521 ; Loss:  0.4095146059989929\n",
      "Epochs:  522 ; Loss:  0.4099300801753998\n",
      "Epochs:  523 ; Loss:  0.4101625084877014\n",
      "Epochs:  524 ; Loss:  0.4094230830669403\n",
      "Epochs:  525 ; Loss:  0.40889981389045715\n",
      "Epochs:  526 ; Loss:  0.4089915156364441\n",
      "Epochs:  527 ; Loss:  0.40841177105903625\n",
      "Epochs:  528 ; Loss:  0.4077552855014801\n",
      "Epochs:  529 ; Loss:  0.4079659581184387\n",
      "Epochs:  530 ; Loss:  0.4070718586444855\n",
      "Epochs:  531 ; Loss:  0.4056885242462158\n",
      "Epochs:  532 ; Loss:  0.40603354573249817\n",
      "Epochs:  533 ; Loss:  0.4063475728034973\n",
      "Epochs:  534 ; Loss:  0.4053410291671753\n",
      "Epochs:  535 ; Loss:  0.4039708375930786\n",
      "Epochs:  536 ; Loss:  0.40273478627204895\n",
      "Epochs:  537 ; Loss:  0.4023500084877014\n",
      "Epochs:  538 ; Loss:  0.40238887071609497\n",
      "Epochs:  539 ; Loss:  0.4015754461288452\n",
      "Epochs:  540 ; Loss:  0.40046849846839905\n",
      "Epochs:  541 ; Loss:  0.3994145691394806\n",
      "Epochs:  542 ; Loss:  0.39855441451072693\n",
      "Epochs:  543 ; Loss:  0.398921400308609\n",
      "Epochs:  544 ; Loss:  0.39985164999961853\n",
      "Epochs:  545 ; Loss:  0.39950382709503174\n",
      "Epochs:  546 ; Loss:  0.3982888460159302\n",
      "Epochs:  547 ; Loss:  0.397551566362381\n",
      "Epochs:  548 ; Loss:  0.3970680832862854\n",
      "Epochs:  549 ; Loss:  0.3966342806816101\n",
      "Epochs:  550 ; Loss:  0.3965968191623688\n",
      "Epochs:  551 ; Loss:  0.39653468132019043\n",
      "Epochs:  552 ; Loss:  0.3960489332675934\n",
      "Epochs:  553 ; Loss:  0.39575010538101196\n",
      "Epochs:  554 ; Loss:  0.39551520347595215\n",
      "Epochs:  555 ; Loss:  0.39484331011772156\n",
      "Epochs:  556 ; Loss:  0.39447519183158875\n",
      "Epochs:  557 ; Loss:  0.39460551738739014\n",
      "Epochs:  558 ; Loss:  0.394435316324234\n",
      "Epochs:  559 ; Loss:  0.39353713393211365\n",
      "Epochs:  560 ; Loss:  0.3921007215976715\n",
      "Epochs:  561 ; Loss:  0.3911582827568054\n",
      "Epochs:  562 ; Loss:  0.3913098871707916\n",
      "Epochs:  563 ; Loss:  0.3911851942539215\n",
      "Epochs:  564 ; Loss:  0.39010733366012573\n",
      "Epochs:  565 ; Loss:  0.3891417682170868\n",
      "Epochs:  566 ; Loss:  0.38870730996131897\n",
      "Epochs:  567 ; Loss:  0.3885593116283417\n",
      "Epochs:  568 ; Loss:  0.38828080892562866\n",
      "Epochs:  569 ; Loss:  0.3874795734882355\n",
      "Epochs:  570 ; Loss:  0.38702112436294556\n",
      "Epochs:  571 ; Loss:  0.38702550530433655\n",
      "Epochs:  572 ; Loss:  0.3863697052001953\n",
      "Epochs:  573 ; Loss:  0.38547301292419434\n",
      "Epochs:  574 ; Loss:  0.38529983162879944\n",
      "Epochs:  575 ; Loss:  0.384570837020874\n",
      "Epochs:  576 ; Loss:  0.38319459557533264\n",
      "Epochs:  577 ; Loss:  0.38276442885398865\n",
      "Epochs:  578 ; Loss:  0.383157879114151\n",
      "Epochs:  579 ; Loss:  0.3832160532474518\n",
      "Epochs:  580 ; Loss:  0.38268882036209106\n",
      "Epochs:  581 ; Loss:  0.3821019232273102\n",
      "Epochs:  582 ; Loss:  0.3815981447696686\n",
      "Epochs:  583 ; Loss:  0.3811126947402954\n",
      "Epochs:  584 ; Loss:  0.3809553384780884\n",
      "Epochs:  585 ; Loss:  0.3812428414821625\n",
      "Epochs:  586 ; Loss:  0.38119298219680786\n",
      "Epochs:  587 ; Loss:  0.3806627690792084\n",
      "Epochs:  588 ; Loss:  0.3801707327365875\n",
      "Epochs:  589 ; Loss:  0.37926480174064636\n",
      "Epochs:  590 ; Loss:  0.3778567612171173\n",
      "Epochs:  591 ; Loss:  0.3768659830093384\n",
      "Epochs:  592 ; Loss:  0.3756923973560333\n",
      "Epochs:  593 ; Loss:  0.37476104497909546\n",
      "Epochs:  594 ; Loss:  0.37527525424957275\n",
      "Epochs:  595 ; Loss:  0.37629514932632446\n",
      "Epochs:  596 ; Loss:  0.376008003950119\n",
      "Epochs:  597 ; Loss:  0.37496745586395264\n",
      "Epochs:  598 ; Loss:  0.3744628131389618\n",
      "Epochs:  599 ; Loss:  0.37421920895576477\n",
      "Epochs:  600 ; Loss:  0.37381941080093384\n",
      "Epochs:  601 ; Loss:  0.3734210729598999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:  602 ; Loss:  0.3727775514125824\n",
      "Epochs:  603 ; Loss:  0.37265777587890625\n",
      "Epochs:  604 ; Loss:  0.37279707193374634\n",
      "Epochs:  605 ; Loss:  0.37125539779663086\n",
      "Epochs:  606 ; Loss:  0.3694111108779907\n",
      "Epochs:  607 ; Loss:  0.3691807687282562\n",
      "Epochs:  608 ; Loss:  0.36889931559562683\n",
      "Epochs:  609 ; Loss:  0.36830711364746094\n",
      "Epochs:  610 ; Loss:  0.36868175864219666\n",
      "Epochs:  611 ; Loss:  0.36914926767349243\n",
      "Epochs:  612 ; Loss:  0.36905932426452637\n",
      "Epochs:  613 ; Loss:  0.3690318167209625\n",
      "Epochs:  614 ; Loss:  0.36873024702072144\n",
      "Epochs:  615 ; Loss:  0.3673945367336273\n",
      "Epochs:  616 ; Loss:  0.3664090633392334\n",
      "Epochs:  617 ; Loss:  0.3664132058620453\n",
      "Epochs:  618 ; Loss:  0.3658519983291626\n",
      "Epochs:  619 ; Loss:  0.3647475838661194\n",
      "Epochs:  620 ; Loss:  0.3644118010997772\n",
      "Epochs:  621 ; Loss:  0.36457186937332153\n",
      "Epochs:  622 ; Loss:  0.36479616165161133\n",
      "Epochs:  623 ; Loss:  0.36429521441459656\n",
      "Epochs:  624 ; Loss:  0.3627541661262512\n",
      "Epochs:  625 ; Loss:  0.3613343834877014\n",
      "Epochs:  626 ; Loss:  0.36127057671546936\n",
      "Epochs:  627 ; Loss:  0.3614799380302429\n",
      "Epochs:  628 ; Loss:  0.3607143461704254\n",
      "Epochs:  629 ; Loss:  0.35933342576026917\n",
      "Epochs:  630 ; Loss:  0.35823458433151245\n",
      "Epochs:  631 ; Loss:  0.35766032338142395\n",
      "Epochs:  632 ; Loss:  0.35806405544281006\n",
      "Epochs:  633 ; Loss:  0.3588951826095581\n",
      "Epochs:  634 ; Loss:  0.35860535502433777\n",
      "Epochs:  635 ; Loss:  0.3572753071784973\n",
      "Epochs:  636 ; Loss:  0.3563500940799713\n",
      "Epochs:  637 ; Loss:  0.35571324825286865\n",
      "Epochs:  638 ; Loss:  0.3546891510486603\n",
      "Epochs:  639 ; Loss:  0.3541651964187622\n",
      "Epochs:  640 ; Loss:  0.3539346754550934\n",
      "Epochs:  641 ; Loss:  0.35287153720855713\n",
      "Epochs:  642 ; Loss:  0.3517060875892639\n",
      "Epochs:  643 ; Loss:  0.3516344428062439\n",
      "Epochs:  644 ; Loss:  0.35248103737831116\n",
      "Epochs:  645 ; Loss:  0.353107213973999\n",
      "Epochs:  646 ; Loss:  0.3525865972042084\n",
      "Epochs:  647 ; Loss:  0.3515965938568115\n",
      "Epochs:  648 ; Loss:  0.35079193115234375\n",
      "Epochs:  649 ; Loss:  0.34992727637290955\n",
      "Epochs:  650 ; Loss:  0.3490794599056244\n",
      "Epochs:  651 ; Loss:  0.348837673664093\n",
      "Epochs:  652 ; Loss:  0.3493826687335968\n",
      "Epochs:  653 ; Loss:  0.34992194175720215\n",
      "Epochs:  654 ; Loss:  0.3493261933326721\n",
      "Epochs:  655 ; Loss:  0.34773215651512146\n",
      "Epochs:  656 ; Loss:  0.34658434987068176\n",
      "Epochs:  657 ; Loss:  0.3462234437465668\n",
      "Epochs:  658 ; Loss:  0.3455958962440491\n",
      "Epochs:  659 ; Loss:  0.34463492035865784\n",
      "Epochs:  660 ; Loss:  0.3437959551811218\n",
      "Epochs:  661 ; Loss:  0.3434235453605652\n",
      "Epochs:  662 ; Loss:  0.34368258714675903\n",
      "Epochs:  663 ; Loss:  0.34352704882621765\n",
      "Epochs:  664 ; Loss:  0.3422181010246277\n",
      "Epochs:  665 ; Loss:  0.34110188484191895\n",
      "Epochs:  666 ; Loss:  0.3410531282424927\n",
      "Epochs:  667 ; Loss:  0.34129366278648376\n",
      "Epochs:  668 ; Loss:  0.34134072065353394\n",
      "Epochs:  669 ; Loss:  0.34152474999427795\n",
      "Epochs:  670 ; Loss:  0.34121379256248474\n",
      "Epochs:  671 ; Loss:  0.3398749530315399\n",
      "Epochs:  672 ; Loss:  0.33854424953460693\n",
      "Epochs:  673 ; Loss:  0.3381465971469879\n",
      "Epochs:  674 ; Loss:  0.33854615688323975\n",
      "Epochs:  675 ; Loss:  0.3388664126396179\n",
      "Epochs:  676 ; Loss:  0.3380576968193054\n",
      "Epochs:  677 ; Loss:  0.3368344008922577\n",
      "Epochs:  678 ; Loss:  0.33658567070961\n",
      "Epochs:  679 ; Loss:  0.33675065636634827\n",
      "Epochs:  680 ; Loss:  0.33667638897895813\n",
      "Epochs:  681 ; Loss:  0.33652999997138977\n",
      "Epochs:  682 ; Loss:  0.33548739552497864\n",
      "Epochs:  683 ; Loss:  0.3343593180179596\n",
      "Epochs:  684 ; Loss:  0.3347409963607788\n",
      "Epochs:  685 ; Loss:  0.33443716168403625\n",
      "Epochs:  686 ; Loss:  0.3328493535518646\n",
      "Epochs:  687 ; Loss:  0.33212006092071533\n",
      "Epochs:  688 ; Loss:  0.33169302344322205\n",
      "Epochs:  689 ; Loss:  0.3302895426750183\n",
      "Epochs:  690 ; Loss:  0.32863929867744446\n",
      "Epochs:  691 ; Loss:  0.3280821740627289\n",
      "Epochs:  692 ; Loss:  0.3293071687221527\n",
      "Epochs:  693 ; Loss:  0.33037975430488586\n",
      "Epochs:  694 ; Loss:  0.32932931184768677\n",
      "Epochs:  695 ; Loss:  0.3280958831310272\n",
      "Epochs:  696 ; Loss:  0.32783591747283936\n",
      "Epochs:  697 ; Loss:  0.32730597257614136\n",
      "Epochs:  698 ; Loss:  0.32743263244628906\n",
      "Epochs:  699 ; Loss:  0.32806867361068726\n",
      "Epochs:  700 ; Loss:  0.3265729546546936\n",
      "Epochs:  701 ; Loss:  0.3244433104991913\n",
      "Epochs:  702 ; Loss:  0.32421138882637024\n",
      "Epochs:  703 ; Loss:  0.32448068261146545\n",
      "Epochs:  704 ; Loss:  0.32414552569389343\n",
      "Epochs:  705 ; Loss:  0.32368236780166626\n",
      "Epochs:  706 ; Loss:  0.3235633969306946\n",
      "Epochs:  707 ; Loss:  0.3236503601074219\n",
      "Epochs:  708 ; Loss:  0.32280322909355164\n",
      "Epochs:  709 ; Loss:  0.3218574821949005\n",
      "Epochs:  710 ; Loss:  0.32281866669654846\n",
      "Epochs:  711 ; Loss:  0.32338860630989075\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(loss\u001b[38;5;241m>\u001b[39mtol):\n\u001b[0;32m     10\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     14\u001b[0m         y_pred\u001b[38;5;241m=\u001b[39mmodel(x_batch)\n\u001b[0;32m     15\u001b[0m         loss\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(y_pred, y_batch)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    560\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 561\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    563\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:84\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     83\u001b[0m     transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:84\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     83\u001b[0m     transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:56\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     54\u001b[0m         storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_new_shared(numel)\n\u001b[0;32m     55\u001b[0m         out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr_\u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring_\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndarray\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemmap\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_epoch=[]\n",
    "loss_values = []\n",
    "loss=1\n",
    "tol=1e-7\n",
    "epochs=0\n",
    "\n",
    "print(\"Epochs    Loss\")\n",
    "\n",
    "while(loss>tol):\n",
    "    epochs=epochs+1\n",
    "    \n",
    "    for x_batch, y_batch in loader:\n",
    "        # Forward pass\n",
    "        y_pred=model(x_batch)\n",
    "        loss=torch.nn.functional.mse_loss(y_pred, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Weights\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    loss_epoch.append(epochs)\n",
    "    loss_values.append(loss.item())\n",
    "    \n",
    "    if epochs%1==0:\n",
    "        print(\"Epochs: \", epochs, \"; Loss: \", loss.item())\n",
    "        \n",
    "    loss=loss.item()\n",
    "\n",
    "print(epochs, \"    \", loss.item())\n",
    "\n",
    "#Plot loss function\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(loss_epoch, loss_values)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518e3c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
