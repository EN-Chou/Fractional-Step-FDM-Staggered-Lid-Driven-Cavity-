{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67f524e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66145b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda:1') #先調1再調0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92194826",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.randn(1000, 6724)\n",
    "x=x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8bf7a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 6724])\n",
      "torch.Size([8000, 6724])\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Prepare data-1/2\n",
    "# DataLoader wraps a Dataset and provides minibatches, shuffling, multithreading, for you\n",
    "data_in=np.loadtxt('./data/data_reserved/Re_100/input_u_fake.dat')\n",
    "data_out=np.loadtxt('./data/data_reserved/Re_100/output_p.dat')\n",
    "x=torch.Tensor(data_in)\n",
    "y=torch.Tensor(data_out)\n",
    "x=x.to(device)\n",
    "y=y.to(device)\n",
    "x=x[:8000]\n",
    "y=y[:8000]\n",
    "print(x.size())\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27fe9583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Create model 建立model習慣建立class\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, B, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear_1=torch.nn.Linear(D_in, H)\n",
    "        self.linear_2=torch.nn.Linear(H, B)\n",
    "        self.linear_3=torch.nn.Linear(B, D_out)\n",
    "    \n",
    "    # Step 3. Forward pass-1/2    # Step 4. Backward pass-1/2\n",
    "    def forward(self, x):\n",
    "        h=self.linear_1(x)\n",
    "        h_relu=torch.nn.functional.relu(h) #為何activation and hidden layer 的實現方式不同\n",
    "        b=self.linear_2(h_relu) \n",
    "        b_relu=torch.nn.functional.relu(b)\n",
    "        y_pred=self.linear_3(b_relu) \n",
    "        return y_pred\n",
    "    \n",
    "model= TwoLayerNet(D_in=6400, H=1000, B=1000, D_out=6400)\n",
    "model=model.to(device) #這行是什麼意思? A:将模型加载到相应的设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5195ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.nn.Sequential(torch.nn.Unflatten(1, (1,82,82)))(x)\n",
    "# Convolutional neural network (two convolutional layers)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self, channel_1, channel_2, kernel_dim):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, channel_1, kernel_dim)\n",
    "        self.conv2 = nn.Conv2d(channel_1, channel_2, kernel_dim)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(78*78*3, 1000)  # 78*78 from image dimension\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 6724)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = ConvNet(channel_1=3, channel_2=3, kernel_dim=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45c2379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=DataLoader(TensorDataset(x, y), batch_size=200)\n",
    "tol=1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03232956",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f74471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5de301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs    Loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ENCHOU\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:  1 ; Loss:  8.508725295541808e-05\n",
      "Epochs:  2 ; Loss:  2.906091140175704e-06\n",
      "Epochs:  3 ; Loss:  5.852918548043817e-05\n",
      "Epochs:  4 ; Loss:  6.180214768392034e-06\n",
      "Epochs:  5 ; Loss:  0.0001799257006496191\n",
      "Epochs:  6 ; Loss:  4.879911443822493e-07\n",
      "Epochs:  7 ; Loss:  0.00046651175944134593\n",
      "Epochs:  8 ; Loss:  3.703044910707831e-07\n",
      "Epochs:  9 ; Loss:  0.00017881119856610894\n",
      "Epochs:  10 ; Loss:  1.7290607502218336e-05\n",
      "Epochs:  11 ; Loss:  2.81322115824878e-07\n",
      "Epochs:  12 ; Loss:  0.0002774156746454537\n",
      "Epochs:  13 ; Loss:  4.1002266470968607e-07\n",
      "Epochs:  14 ; Loss:  3.42422083576821e-07\n",
      "Epochs:  15 ; Loss:  2.998040713464434e-07\n",
      "Epochs:  16 ; Loss:  6.955381104489788e-05\n",
      "Epochs:  17 ; Loss:  2.2962036894114135e-07\n",
      "Epochs:  18 ; Loss:  2.2320921289065154e-07\n",
      "Epochs:  19 ; Loss:  2.1123636884112784e-07\n",
      "Epochs:  20 ; Loss:  1.9296457764994557e-07\n",
      "Epochs:  21 ; Loss:  1.733291128402925e-07\n",
      "Epochs:  22 ; Loss:  1.7332381219148374e-07\n",
      "Epochs:  23 ; Loss:  2.6031833044726227e-07\n",
      "Epochs:  24 ; Loss:  6.8644300199594e-07\n",
      "Epochs:  25 ; Loss:  7.753298268653452e-05\n",
      "Epochs:  26 ; Loss:  2.1103730887261918e-06\n",
      "Epochs:  27 ; Loss:  1.2255666206328897e-06\n",
      "Epochs:  28 ; Loss:  4.004432776127942e-05\n",
      "Epochs:  29 ; Loss:  4.652865115417626e-08\n",
      "Epochs:  30 ; Loss:  5.859834928401142e-08\n",
      "Epochs:  31 ; Loss:  6.639744043468454e-08\n",
      "Epochs:  32 ; Loss:  7.046484284956023e-08\n",
      "Epochs:  33 ; Loss:  1.6013034837669693e-05\n",
      "Epochs:  34 ; Loss:  8.531684159152064e-08\n",
      "Epochs:  35 ; Loss:  1.5006052308308426e-05\n",
      "Epochs:  36 ; Loss:  8.228408887589467e-08\n",
      "Epochs:  37 ; Loss:  4.207827805657871e-05\n",
      "Epochs:  38 ; Loss:  8.742153312368828e-08\n",
      "Epochs:  39 ; Loss:  0.00013729736383538693\n",
      "Epochs:  40 ; Loss:  2.877463884942699e-07\n",
      "Epochs:  41 ; Loss:  6.256362394196913e-05\n",
      "Epochs:  42 ; Loss:  1.8287016700924141e-06\n",
      "Epochs:  43 ; Loss:  0.00010298249253537506\n",
      "Epochs:  44 ; Loss:  1.2103886604108993e-07\n",
      "Epochs:  45 ; Loss:  3.85156599804759e-05\n",
      "Epochs:  46 ; Loss:  1.8624827134772204e-05\n",
      "Epochs:  47 ; Loss:  1.0136005585081875e-05\n",
      "Epochs:  48 ; Loss:  1.9521105798503413e-07\n",
      "Epochs:  49 ; Loss:  8.666045658856092e-08\n",
      "Epochs:  50 ; Loss:  9.220578789381761e-08\n",
      "Epochs:  51 ; Loss:  1.1132046751072266e-07\n",
      "Epochs:  52 ; Loss:  2.4150233457476133e-06\n",
      "Epochs:  53 ; Loss:  7.204875146271661e-06\n",
      "Epochs:  54 ; Loss:  3.061187044295366e-06\n",
      "Epochs:  55 ; Loss:  8.080074621830136e-06\n",
      "Epochs:  56 ; Loss:  1.0298012966813985e-05\n",
      "Epochs:  57 ; Loss:  8.596610001632143e-08\n",
      "Epochs:  58 ; Loss:  7.529827428243152e-08\n",
      "Epochs:  59 ; Loss:  7.554559033451369e-06\n",
      "Epochs:  60 ; Loss:  9.968535778170917e-06\n",
      "Epochs:  61 ; Loss:  1.2638975931622554e-05\n",
      "Epochs:  62 ; Loss:  2.0632566588574264e-07\n",
      "Epochs:  63 ; Loss:  1.4346160241984762e-05\n",
      "Epochs:  64 ; Loss:  8.894054190022871e-06\n",
      "Epochs:  65 ; Loss:  3.559005563147366e-06\n",
      "Epochs:  66 ; Loss:  1.28607653095969e-05\n",
      "Epochs:  67 ; Loss:  3.5314644719619537e-06\n",
      "Epochs:  68 ; Loss:  1.2385850823193323e-05\n",
      "Epochs:  69 ; Loss:  9.74957515609276e-07\n",
      "Epochs:  70 ; Loss:  8.682950465299655e-07\n",
      "Epochs:  71 ; Loss:  6.712059530400438e-08\n",
      "Epochs:  72 ; Loss:  4.4381374664226314e-07\n",
      "Epochs:  73 ; Loss:  1.3228313946456183e-06\n",
      "Epochs:  74 ; Loss:  1.4604296438847086e-06\n",
      "Epochs:  75 ; Loss:  1.3643741567648249e-06\n",
      "Epochs:  76 ; Loss:  1.4177657021718915e-06\n",
      "Epochs:  77 ; Loss:  1.4622631852034829e-06\n",
      "Epochs:  78 ; Loss:  1.3034198218520032e-06\n",
      "Epochs:  79 ; Loss:  1.5692706938352785e-06\n",
      "Epochs:  80 ; Loss:  1.4123927485343302e-06\n",
      "Epochs:  81 ; Loss:  1.4876484328851802e-06\n",
      "Epochs:  82 ; Loss:  1.487391273258254e-06\n",
      "Epochs:  83 ; Loss:  1.594725745235337e-06\n",
      "Epochs:  84 ; Loss:  1.3709189943256206e-06\n",
      "Epochs:  85 ; Loss:  1.5838010085644783e-06\n",
      "Epochs:  86 ; Loss:  1.4459657222687383e-06\n",
      "Epochs:  87 ; Loss:  1.5502557744184742e-06\n",
      "Epochs:  88 ; Loss:  1.4707579794048797e-06\n",
      "Epochs:  89 ; Loss:  1.6317781046382152e-06\n",
      "Epochs:  90 ; Loss:  1.516710995019821e-06\n",
      "Epochs:  91 ; Loss:  1.5592331692459993e-06\n",
      "Epochs:  92 ; Loss:  1.5085815903148614e-06\n",
      "Epochs:  93 ; Loss:  1.5802311281731818e-06\n",
      "Epochs:  94 ; Loss:  1.4639075516242883e-06\n",
      "Epochs:  95 ; Loss:  1.6089546761577367e-06\n",
      "Epochs:  96 ; Loss:  1.4910002619217266e-06\n",
      "Epochs:  97 ; Loss:  1.6024187061702833e-06\n",
      "Epochs:  98 ; Loss:  1.476837837799394e-06\n",
      "Epochs:  99 ; Loss:  1.604200974725245e-06\n",
      "Epochs:  100 ; Loss:  1.4811304254180868e-06\n",
      "Epochs:  101 ; Loss:  1.5699866935392492e-06\n",
      "Epochs:  102 ; Loss:  1.5218139424177934e-06\n",
      "Epochs:  103 ; Loss:  1.5697501112299506e-06\n",
      "Epochs:  104 ; Loss:  1.5280853631338687e-06\n",
      "Epochs:  105 ; Loss:  1.6502101516380208e-06\n",
      "Epochs:  106 ; Loss:  1.4458337318501435e-06\n",
      "Epochs:  107 ; Loss:  1.618228111510689e-06\n",
      "Epochs:  108 ; Loss:  1.5116722806851612e-06\n",
      "Epochs:  109 ; Loss:  1.5871606819928274e-06\n",
      "Epochs:  110 ; Loss:  1.4412933069252176e-06\n",
      "Epochs:  111 ; Loss:  1.6549350903005688e-06\n",
      "Epochs:  112 ; Loss:  1.5109542346181115e-06\n",
      "Epochs:  113 ; Loss:  1.5881088302194257e-06\n",
      "Epochs:  114 ; Loss:  1.5094583432073705e-06\n",
      "Epochs:  115 ; Loss:  1.5907020269878558e-06\n",
      "Epochs:  116 ; Loss:  1.469190920033725e-06\n",
      "Epochs:  117 ; Loss:  1.6488571645822958e-06\n",
      "Epochs:  118 ; Loss:  1.4969399444453302e-06\n",
      "Epochs:  119 ; Loss:  1.6018100268411217e-06\n",
      "Epochs:  120 ; Loss:  1.4982265383878257e-06\n",
      "Epochs:  121 ; Loss:  1.5975413134583505e-06\n",
      "Epochs:  122 ; Loss:  1.4988537486715359e-06\n",
      "Epochs:  123 ; Loss:  1.6482214277857565e-06\n",
      "Epochs:  124 ; Loss:  1.421229740117269e-06\n",
      "Epochs:  125 ; Loss:  1.6059904055509833e-06\n",
      "Epochs:  126 ; Loss:  1.5613187542840024e-06\n",
      "Epochs:  127 ; Loss:  1.5876400993874995e-06\n",
      "Epochs:  128 ; Loss:  1.4572573263649247e-06\n",
      "Epochs:  129 ; Loss:  1.6280212093988666e-06\n",
      "Epochs:  130 ; Loss:  1.501156589256425e-06\n",
      "Epochs:  131 ; Loss:  1.5562375210720347e-06\n",
      "Epochs:  132 ; Loss:  1.5761903569000424e-06\n",
      "Epochs:  133 ; Loss:  1.5706273188698106e-06\n",
      "Epochs:  134 ; Loss:  1.5080364619279862e-06\n",
      "Epochs:  135 ; Loss:  1.6283222521451535e-06\n",
      "Epochs:  136 ; Loss:  1.495416313446185e-06\n",
      "Epochs:  137 ; Loss:  1.574264615555876e-06\n",
      "Epochs:  138 ; Loss:  1.4872974816171336e-06\n",
      "Epochs:  139 ; Loss:  1.677089699114731e-06\n",
      "Epochs:  140 ; Loss:  1.4431695944949752e-06\n",
      "Epochs:  141 ; Loss:  1.6277988379442832e-06\n",
      "Epochs:  142 ; Loss:  1.4928896234778222e-06\n",
      "Epochs:  143 ; Loss:  1.5830635220481781e-06\n",
      "Epochs:  144 ; Loss:  1.5176198076005676e-06\n",
      "Epochs:  145 ; Loss:  1.6033003475968144e-06\n",
      "Epochs:  146 ; Loss:  1.5368420918093761e-06\n",
      "Epochs:  147 ; Loss:  1.635217927287158e-06\n",
      "Epochs:  148 ; Loss:  1.4569502582162386e-06\n",
      "Epochs:  149 ; Loss:  1.6402885876232176e-06\n",
      "Epochs:  150 ; Loss:  1.4855244216960273e-06\n",
      "Epochs:  151 ; Loss:  1.6076706970125088e-06\n",
      "Epochs:  152 ; Loss:  1.4866496940157958e-06\n",
      "Epochs:  153 ; Loss:  1.6556024320379947e-06\n",
      "Epochs:  154 ; Loss:  1.4651431001766468e-06\n",
      "Epochs:  155 ; Loss:  1.6207648059207713e-06\n",
      "Epochs:  156 ; Loss:  1.4970905795053113e-06\n",
      "Epochs:  157 ; Loss:  1.5978412193362601e-06\n",
      "Epochs:  158 ; Loss:  1.494396201451309e-06\n",
      "Epochs:  159 ; Loss:  1.6086581808849587e-06\n",
      "Epochs:  160 ; Loss:  1.512715357421257e-06\n",
      "Epochs:  161 ; Loss:  1.5973711242622812e-06\n",
      "Epochs:  162 ; Loss:  1.5371102790595614e-06\n",
      "Epochs:  163 ; Loss:  1.6058228311521816e-06\n",
      "Epochs:  164 ; Loss:  1.4601886277887388e-06\n",
      "Epochs:  165 ; Loss:  1.6139605349962949e-06\n",
      "Epochs:  166 ; Loss:  1.4843352573734592e-06\n",
      "Epochs:  167 ; Loss:  1.6582011994614732e-06\n",
      "Epochs:  168 ; Loss:  1.4983396567913587e-06\n",
      "Epochs:  169 ; Loss:  1.5798148069734452e-06\n",
      "Epochs:  170 ; Loss:  1.5021408898974187e-06\n",
      "Epochs:  171 ; Loss:  1.6422879980382277e-06\n",
      "Epochs:  172 ; Loss:  1.4733919897480519e-06\n",
      "Epochs:  173 ; Loss:  1.6274282188533107e-06\n",
      "Epochs:  174 ; Loss:  1.5179033425738453e-06\n",
      "Epochs:  175 ; Loss:  1.6260919437627308e-06\n",
      "Epochs:  176 ; Loss:  1.486644350734423e-06\n",
      "Epochs:  177 ; Loss:  1.618494707145146e-06\n",
      "Epochs:  178 ; Loss:  1.5059797533467645e-06\n",
      "Epochs:  179 ; Loss:  1.6207860653594253e-06\n",
      "Epochs:  180 ; Loss:  1.506151875219075e-06\n",
      "Epochs:  181 ; Loss:  1.6226783827733016e-06\n",
      "Epochs:  182 ; Loss:  1.4963226249165018e-06\n",
      "Epochs:  183 ; Loss:  1.6397818853874924e-06\n",
      "Epochs:  184 ; Loss:  1.4613092389481608e-06\n",
      "Epochs:  185 ; Loss:  1.6200243635466904e-06\n",
      "Epochs:  186 ; Loss:  1.4962598697820795e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:  187 ; Loss:  1.6174606116692303e-06\n",
      "Epochs:  188 ; Loss:  1.5070633025970892e-06\n",
      "Epochs:  189 ; Loss:  1.6862944676176994e-06\n",
      "Epochs:  190 ; Loss:  1.445913198949711e-06\n",
      "Epochs:  191 ; Loss:  1.621256615180755e-06\n",
      "Epochs:  192 ; Loss:  1.5029743281047558e-06\n",
      "Epochs:  193 ; Loss:  1.5852374417590909e-06\n",
      "Epochs:  194 ; Loss:  1.4813471125307842e-06\n",
      "Epochs:  195 ; Loss:  1.6532633253518725e-06\n",
      "Epochs:  196 ; Loss:  1.525608809060941e-06\n",
      "Epochs:  197 ; Loss:  1.5656851246603765e-06\n",
      "Epochs:  198 ; Loss:  1.532586793473456e-06\n",
      "Epochs:  199 ; Loss:  1.61866216785711e-06\n",
      "Epochs:  200 ; Loss:  1.5083635389601113e-06\n",
      "Epochs:  201 ; Loss:  1.60888714617613e-06\n",
      "Epochs:  202 ; Loss:  1.5093930869625183e-06\n",
      "Epochs:  203 ; Loss:  1.608886350368266e-06\n",
      "Epochs:  204 ; Loss:  1.458711039958871e-06\n",
      "Epochs:  205 ; Loss:  1.6470721675432287e-06\n",
      "Epochs:  206 ; Loss:  1.49579079788964e-06\n",
      "Epochs:  207 ; Loss:  1.627239385015855e-06\n",
      "Epochs:  208 ; Loss:  1.4853994798613712e-06\n",
      "Epochs:  209 ; Loss:  1.63085076110292e-06\n",
      "Epochs:  210 ; Loss:  1.4807866364208166e-06\n",
      "Epochs:  211 ; Loss:  1.618384089852043e-06\n",
      "Epochs:  212 ; Loss:  1.5250535625455086e-06\n",
      "Epochs:  213 ; Loss:  1.592460876054247e-06\n",
      "Epochs:  214 ; Loss:  1.491401917519397e-06\n",
      "Epochs:  215 ; Loss:  1.6076371593953809e-06\n",
      "Epochs:  216 ; Loss:  1.5434645774803357e-06\n",
      "Epochs:  217 ; Loss:  1.5973093923093984e-06\n",
      "Epochs:  218 ; Loss:  1.4590774526368477e-06\n",
      "Epochs:  219 ; Loss:  1.6476371911267051e-06\n",
      "Epochs:  220 ; Loss:  1.4681635320812347e-06\n",
      "Epochs:  221 ; Loss:  1.6121022099468973e-06\n",
      "Epochs:  222 ; Loss:  1.501888391430839e-06\n",
      "Epochs:  223 ; Loss:  1.6635970041534165e-06\n",
      "Epochs:  224 ; Loss:  1.4692262766402564e-06\n",
      "Epochs:  225 ; Loss:  1.622913828214223e-06\n",
      "Epochs:  226 ; Loss:  1.5273766393875121e-06\n",
      "Epochs:  227 ; Loss:  1.605460397513525e-06\n",
      "Epochs:  228 ; Loss:  1.4670389418824925e-06\n",
      "Epochs:  229 ; Loss:  1.6351980320905568e-06\n",
      "Epochs:  230 ; Loss:  1.5218934095173609e-06\n",
      "Epochs:  231 ; Loss:  1.5905401369309402e-06\n",
      "Epochs:  232 ; Loss:  1.4872837255097693e-06\n",
      "Epochs:  233 ; Loss:  1.6487973653056542e-06\n",
      "Epochs:  234 ; Loss:  1.5352932223322568e-06\n",
      "Epochs:  235 ; Loss:  1.5661064480809728e-06\n",
      "Epochs:  236 ; Loss:  1.4725651453773025e-06\n",
      "Epochs:  237 ; Loss:  1.6796475392766297e-06\n",
      "Epochs:  238 ; Loss:  1.4846349358776934e-06\n",
      "Epochs:  239 ; Loss:  1.5905617374301073e-06\n",
      "Epochs:  240 ; Loss:  1.489820306233014e-06\n",
      "Epochs:  241 ; Loss:  1.6309620605170494e-06\n",
      "Epochs:  242 ; Loss:  1.4990528143243864e-06\n",
      "Epochs:  243 ; Loss:  1.5951019349813578e-06\n",
      "Epochs:  244 ; Loss:  1.550012029838399e-06\n",
      "Epochs:  245 ; Loss:  1.6096832951006945e-06\n",
      "Epochs:  246 ; Loss:  1.4617671695305035e-06\n",
      "Epochs:  247 ; Loss:  1.6274181007247535e-06\n",
      "Epochs:  248 ; Loss:  1.5205864656309132e-06\n",
      "Epochs:  249 ; Loss:  1.6125648016895866e-06\n",
      "Epochs:  250 ; Loss:  1.4589760439776e-06\n",
      "Epochs:  251 ; Loss:  1.6797442867755308e-06\n",
      "Epochs:  252 ; Loss:  1.5055227322591236e-06\n",
      "Epochs:  253 ; Loss:  1.554975938233838e-06\n",
      "Epochs:  254 ; Loss:  1.5170278402365511e-06\n",
      "Epochs:  255 ; Loss:  1.6059702829807065e-06\n",
      "Epochs:  256 ; Loss:  1.4922702575859148e-06\n",
      "Epochs:  257 ; Loss:  1.6161956182259019e-06\n",
      "Epochs:  258 ; Loss:  1.5531110193478526e-06\n",
      "Epochs:  259 ; Loss:  1.5992488897609292e-06\n",
      "Epochs:  260 ; Loss:  1.4437329127758858e-06\n",
      "Epochs:  261 ; Loss:  1.6827522131279693e-06\n",
      "Epochs:  262 ; Loss:  1.4959389318391914e-06\n",
      "Epochs:  263 ; Loss:  1.5866504554651328e-06\n",
      "Epochs:  264 ; Loss:  1.480654191254871e-06\n",
      "Epochs:  265 ; Loss:  1.6906532209759462e-06\n",
      "Epochs:  266 ; Loss:  1.485124357714085e-06\n",
      "Epochs:  267 ; Loss:  1.5978772580638179e-06\n",
      "Epochs:  268 ; Loss:  1.5099445818123058e-06\n",
      "Epochs:  269 ; Loss:  1.6009190630938974e-06\n",
      "Epochs:  270 ; Loss:  1.524932486063335e-06\n",
      "Epochs:  271 ; Loss:  1.5630856751158717e-06\n",
      "Epochs:  272 ; Loss:  1.5491561953240307e-06\n",
      "Epochs:  273 ; Loss:  1.5910987940515042e-06\n",
      "Epochs:  274 ; Loss:  1.5052180515340297e-06\n",
      "Epochs:  275 ; Loss:  1.6041701655922225e-06\n",
      "Epochs:  276 ; Loss:  1.4928922382750898e-06\n",
      "Epochs:  277 ; Loss:  1.6245257938862778e-06\n",
      "Epochs:  278 ; Loss:  1.462955310671532e-06\n",
      "Epochs:  279 ; Loss:  1.666126308919047e-06\n",
      "Epochs:  280 ; Loss:  1.4642237147199921e-06\n",
      "Epochs:  281 ; Loss:  1.6137162219820311e-06\n",
      "Epochs:  282 ; Loss:  1.512944891146617e-06\n",
      "Epochs:  283 ; Loss:  1.571180860082677e-06\n",
      "Epochs:  284 ; Loss:  1.4725952723892988e-06\n",
      "Epochs:  285 ; Loss:  1.6250442058662884e-06\n",
      "Epochs:  286 ; Loss:  1.541367851132236e-06\n",
      "Epochs:  287 ; Loss:  1.574533712300763e-06\n",
      "Epochs:  288 ; Loss:  1.5005028899395256e-06\n",
      "Epochs:  289 ; Loss:  1.6341987247869838e-06\n",
      "Epochs:  290 ; Loss:  1.4722763808094896e-06\n",
      "Epochs:  291 ; Loss:  1.6123271961987484e-06\n",
      "Epochs:  292 ; Loss:  1.4900255109751015e-06\n",
      "Epochs:  293 ; Loss:  1.6313471178364125e-06\n",
      "Epochs:  294 ; Loss:  1.4986424048402114e-06\n",
      "Epochs:  295 ; Loss:  1.580058665240358e-06\n",
      "Epochs:  296 ; Loss:  1.5113466815819265e-06\n",
      "Epochs:  297 ; Loss:  1.651337584007706e-06\n",
      "Epochs:  298 ; Loss:  1.4702814041811507e-06\n",
      "Epochs:  299 ; Loss:  1.614180632714124e-06\n",
      "Epochs:  300 ; Loss:  1.513978531875182e-06\n",
      "Epochs:  301 ; Loss:  1.6076287465693895e-06\n",
      "Epochs:  302 ; Loss:  1.4939639640942914e-06\n",
      "Epochs:  303 ; Loss:  1.616247459423903e-06\n",
      "Epochs:  304 ; Loss:  1.4654680171588552e-06\n",
      "Epochs:  305 ; Loss:  1.5900986909400672e-06\n",
      "Epochs:  306 ; Loss:  1.5454053254870814e-06\n",
      "Epochs:  307 ; Loss:  1.6044458561736974e-06\n",
      "Epochs:  308 ; Loss:  1.4770932921237545e-06\n",
      "Epochs:  309 ; Loss:  1.6414801393693779e-06\n",
      "Epochs:  310 ; Loss:  1.52478105519549e-06\n",
      "Epochs:  311 ; Loss:  1.603789314685855e-06\n",
      "Epochs:  312 ; Loss:  1.4782312973693479e-06\n",
      "Epochs:  313 ; Loss:  1.6398524849137175e-06\n",
      "Epochs:  314 ; Loss:  1.49932657222962e-06\n",
      "Epochs:  315 ; Loss:  1.620777197786083e-06\n",
      "Epochs:  316 ; Loss:  1.491314151280676e-06\n",
      "Epochs:  317 ; Loss:  1.6442755850221147e-06\n",
      "Epochs:  318 ; Loss:  1.5005510931587196e-06\n",
      "Epochs:  319 ; Loss:  1.5857008293096442e-06\n",
      "Epochs:  320 ; Loss:  1.4751025219084113e-06\n",
      "Epochs:  321 ; Loss:  1.6868964394234354e-06\n",
      "Epochs:  322 ; Loss:  1.5017974419606617e-06\n",
      "Epochs:  323 ; Loss:  1.5993869055819232e-06\n",
      "Epochs:  324 ; Loss:  1.522408751952753e-06\n",
      "Epochs:  325 ; Loss:  1.576395106894779e-06\n",
      "Epochs:  326 ; Loss:  1.4968679806770524e-06\n",
      "Epochs:  327 ; Loss:  1.6021331248339266e-06\n",
      "Epochs:  328 ; Loss:  1.5472389804926934e-06\n",
      "Epochs:  329 ; Loss:  1.597481855242222e-06\n",
      "Epochs:  330 ; Loss:  1.471123823648668e-06\n",
      "Epochs:  331 ; Loss:  1.6467973864564556e-06\n",
      "Epochs:  332 ; Loss:  1.48234528296598e-06\n",
      "Epochs:  333 ; Loss:  1.6676286804795382e-06\n",
      "Epochs:  334 ; Loss:  1.4574707165593281e-06\n",
      "Epochs:  335 ; Loss:  1.6345575204468332e-06\n",
      "Epochs:  336 ; Loss:  1.4902210523359827e-06\n",
      "Epochs:  337 ; Loss:  1.6087922176666325e-06\n",
      "Epochs:  338 ; Loss:  1.4861229828966316e-06\n",
      "Epochs:  339 ; Loss:  1.62663786795747e-06\n",
      "Epochs:  340 ; Loss:  1.5083811604199582e-06\n",
      "Epochs:  341 ; Loss:  1.5605399994456093e-06\n",
      "Epochs:  342 ; Loss:  1.6424875184384291e-06\n",
      "Epochs:  343 ; Loss:  1.5762335578983766e-06\n",
      "Epochs:  344 ; Loss:  1.4680971389680053e-06\n",
      "Epochs:  345 ; Loss:  1.665613808654598e-06\n",
      "Epochs:  346 ; Loss:  1.4721684920004918e-06\n",
      "Epochs:  347 ; Loss:  1.5878658814472146e-06\n",
      "Epochs:  348 ; Loss:  1.5245988151946221e-06\n",
      "Epochs:  349 ; Loss:  1.6670514924044255e-06\n",
      "Epochs:  350 ; Loss:  1.4822720686424873e-06\n",
      "Epochs:  351 ; Loss:  1.5997754871932557e-06\n",
      "Epochs:  352 ; Loss:  1.5402969211208983e-06\n",
      "Epochs:  353 ; Loss:  1.5883858850429533e-06\n",
      "Epochs:  354 ; Loss:  1.4946560895623406e-06\n",
      "Epochs:  355 ; Loss:  1.6042796460169484e-06\n",
      "Epochs:  356 ; Loss:  1.5211861636998947e-06\n",
      "Epochs:  357 ; Loss:  1.6148787835845724e-06\n",
      "Epochs:  358 ; Loss:  1.4981217191234464e-06\n",
      "Epochs:  359 ; Loss:  1.6139576928253518e-06\n",
      "Epochs:  360 ; Loss:  1.479359980294248e-06\n",
      "Epochs:  361 ; Loss:  1.6359956589440117e-06\n",
      "Epochs:  362 ; Loss:  1.4822318235019338e-06\n",
      "Epochs:  363 ; Loss:  1.6380618035327643e-06\n",
      "Epochs:  364 ; Loss:  1.4695347090309951e-06\n",
      "Epochs:  365 ; Loss:  1.6515065226485603e-06\n",
      "Epochs:  366 ; Loss:  1.4999912991697784e-06\n",
      "Epochs:  367 ; Loss:  1.6055281548688072e-06\n",
      "Epochs:  368 ; Loss:  1.497546463724575e-06\n",
      "Epochs:  369 ; Loss:  1.6274834706564434e-06\n",
      "Epochs:  370 ; Loss:  1.5100097243703203e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:  371 ; Loss:  1.604254521225812e-06\n",
      "Epochs:  372 ; Loss:  1.471104155825742e-06\n",
      "Epochs:  373 ; Loss:  1.6365825104003306e-06\n",
      "Epochs:  374 ; Loss:  1.4732651152371545e-06\n",
      "Epochs:  375 ; Loss:  1.6300185734507977e-06\n",
      "Epochs:  376 ; Loss:  1.5052421531436266e-06\n",
      "Epochs:  377 ; Loss:  1.6584702962063602e-06\n",
      "Epochs:  378 ; Loss:  1.490925342295668e-06\n",
      "Epochs:  379 ; Loss:  1.5906422277112142e-06\n",
      "Epochs:  380 ; Loss:  1.483179175920668e-06\n",
      "Epochs:  381 ; Loss:  1.6262571307379403e-06\n",
      "Epochs:  382 ; Loss:  1.4843935787212104e-06\n",
      "Epochs:  383 ; Loss:  1.6103525695143617e-06\n",
      "Epochs:  384 ; Loss:  1.5255200196406804e-06\n",
      "Epochs:  385 ; Loss:  1.5956599099808955e-06\n",
      "Epochs:  386 ; Loss:  1.492783553658228e-06\n",
      "Epochs:  387 ; Loss:  1.6522263877050136e-06\n",
      "Epochs:  388 ; Loss:  1.447114186703402e-06\n",
      "Epochs:  389 ; Loss:  1.6182234503503423e-06\n",
      "Epochs:  390 ; Loss:  1.4516704140987713e-06\n",
      "Epochs:  391 ; Loss:  1.6686532262610854e-06\n",
      "Epochs:  392 ; Loss:  1.4900483620294835e-06\n",
      "Epochs:  393 ; Loss:  1.5833772977202898e-06\n",
      "Epochs:  394 ; Loss:  1.5188217048489605e-06\n",
      "Epochs:  395 ; Loss:  1.598100993760454e-06\n",
      "Epochs:  396 ; Loss:  1.4945238717700704e-06\n",
      "Epochs:  397 ; Loss:  1.6159382312253001e-06\n",
      "Epochs:  398 ; Loss:  1.5328628251154441e-06\n",
      "Epochs:  399 ; Loss:  1.6162963447641232e-06\n",
      "Epochs:  400 ; Loss:  1.4881162542224047e-06\n",
      "Epochs:  401 ; Loss:  1.6160173572643544e-06\n",
      "Epochs:  402 ; Loss:  1.5337793684011558e-06\n",
      "Epochs:  403 ; Loss:  1.6170264416359714e-06\n",
      "Epochs:  404 ; Loss:  1.4364287608259474e-06\n",
      "Epochs:  405 ; Loss:  1.6837121847856906e-06\n",
      "Epochs:  406 ; Loss:  1.5084182223290554e-06\n",
      "Epochs:  407 ; Loss:  1.5702148630225565e-06\n",
      "Epochs:  408 ; Loss:  1.4950466038499144e-06\n",
      "Epochs:  409 ; Loss:  1.6475328266096767e-06\n",
      "Epochs:  410 ; Loss:  1.4585683629775303e-06\n",
      "Epochs:  411 ; Loss:  1.6058497749327216e-06\n",
      "Epochs:  412 ; Loss:  1.5512885056523373e-06\n",
      "Epochs:  413 ; Loss:  1.5851595662752516e-06\n",
      "Epochs:  414 ; Loss:  1.4922818536433624e-06\n",
      "Epochs:  415 ; Loss:  1.6214731886066147e-06\n",
      "Epochs:  416 ; Loss:  1.5099655001904466e-06\n",
      "Epochs:  417 ; Loss:  1.6288870483549545e-06\n",
      "Epochs:  418 ; Loss:  1.494236926191661e-06\n",
      "Epochs:  419 ; Loss:  1.6189769667107612e-06\n",
      "Epochs:  420 ; Loss:  1.4819084981354536e-06\n",
      "Epochs:  421 ; Loss:  1.5946553730827873e-06\n",
      "Epochs:  422 ; Loss:  1.4987755321271834e-06\n",
      "Epochs:  423 ; Loss:  1.620811531211075e-06\n",
      "Epochs:  424 ; Loss:  1.4812413837717031e-06\n",
      "Epochs:  425 ; Loss:  1.5800382016095682e-06\n",
      "Epochs:  426 ; Loss:  1.5609782622050261e-06\n",
      "Epochs:  427 ; Loss:  1.5890694839981734e-06\n",
      "Epochs:  428 ; Loss:  1.4422954563997337e-06\n",
      "Epochs:  429 ; Loss:  1.6952466239672503e-06\n",
      "Epochs:  430 ; Loss:  1.4796506775383023e-06\n"
     ]
    }
   ],
   "source": [
    "loss_epoch=[]\n",
    "loss_values = []\n",
    "loss=1\n",
    "epochs=0\n",
    "\n",
    "print(\"Epochs    Loss\")\n",
    "\n",
    "while(loss>tol):\n",
    "    epochs=epochs+1\n",
    "    scheduler.step()\n",
    "    \n",
    "    for x_batch, y_batch in loader:\n",
    "        # Forward pass\n",
    "        y_pred=model(x_batch)\n",
    "        loss=torch.nn.functional.mse_loss(y_pred, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Weights\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    loss_epoch.append(epochs)\n",
    "    loss_values.append(loss.item())\n",
    "    \n",
    "    if epochs%1==0:\n",
    "        print(\"Epochs: \", epochs, \"; Loss: \", loss.item())\n",
    "        \n",
    "    loss=loss.item()\n",
    "\n",
    "print(epochs, \"    \", loss.item())\n",
    "\n",
    "#Plot loss function\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(loss_epoch, loss_values)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7643f8cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEGCAYAAACzYDhlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAsBElEQVR4nO3de5wdVZ3v/c+3u9MJCbmnuSWBBAlqUEHtQbyhIwhBPURHHMN4wTkojyOMevCMkKODHsa8RnSeYXQeQBnBg4iGiCB9NBJHAR0QkjS3SIKBJgkk4ZJO50Iu9P33/FGrw+6dvbs7YVd3h3zfr1e/qFq16lerivT+9Vq1dpUiAjMzszxVDXUDzMzslc/JxszMcudkY2ZmuXOyMTOz3DnZmJlZ7mqGugHD0ZQpU2LGjBlD3QwzswPKAw88sDki6kptc7IpYcaMGTQ2Ng51M8zMDiiSniq3zcNoZmaWOycbMzPLnZONmZnlzsnGzMxy52RjZma5c7IxM7Pc5ZpsJM2RtFpSk6RLS2wfKenmtH2ppBkF2+an8tWSzuwvpqSLUllImlJ0nHdLeljSSkm/z+l0zcysjNySjaRq4CrgLGA2cK6k2UXVzge2RsRxwJXAFWnf2cA84ARgDnC1pOp+Yt4LnA70muctaQJwNXB2RJwAfKTCp7rH48/v4F9/s5rNO9vyOoSZ2QEpz57NyUBTRKyJiHZgITC3qM5c4Ia0fAtwmiSl8oUR0RYRa4GmFK9szIh4KCLWlWjH3wC3RsTTqd6mSp5koSee38l372xiy672vA5hZnZAyjPZTAXWF6xvSGUl60REJ7AdmNzHvgOJWex4YKKkuyU9IOmTpSpJukBSo6TG5ubmfkKamdm+OBgmCNQAbwbeD5wJ/KOk44srRcS1EVEfEfV1dSUf7TNgfvmpmVlveT4bbSMwvWB9WiorVWeDpBpgPNDSz779xSy2AWiJiF3ALkl/AE4EHh/4qQyMVOmIZmavDHn2bJYDsyTNlFRLdsO/oahOA3BeWj4HuDMiIpXPS7PVZgKzgGUDjFnsduAdkmokjQbeAjxWgfMzM7MByq1nExGdki4ClgDVwPURsVLS5UBjRDQA1wE3SmoCtpAlD1K9RcAqoBO4MCK6IJviXBwzlX8e+DJwBLBC0uKI+HREPCbpDmAF0A38ICIezeu8zcxsbwrfYNhLfX197M8rBhb/6Vk+d9OD3PHFd/KaI8bl0DIzs+FL0gMRUV9q28EwQWDQ+JaNmVlpTjZmZpY7J5sceGTSzKw3J5sK8tRnM7PSnGzMzCx3TjY58DCamVlvTjYV5XE0M7NSnGzMzCx3TjY5CDyOZmZWyMmmgjwbzcysNCcbMzPLnZONmZnlzskmB576bGbWm5NNBfmWjZlZaU42ZmaWu1yTjaQ5klZLapJ0aYntIyXdnLYvlTSjYNv8VL5a0pn9xZR0USoLSVNKHOsvJHVKOieHUzUzsz7klmwkVQNXAWcBs4FzJc0uqnY+sDUijgOuBK5I+84me2vnCcAc4GpJ1f3EvBc4HXiqTFuuAH5T0ZPc+zh5hjczO2Dl2bM5GWiKiDUR0Q4sBOYW1ZkL3JCWbwFOU/aJPRdYGBFtEbEWaErxysaMiIciYl2Ztvw98HNgU8XOzszMBizPZDMVWF+wviGVlawTEZ3AdmByH/sOJGYvkqYCHwKu6afeBZIaJTU2Nzf3VbVfno1mZtbbwTBB4N+ASyKiu69KEXFtRNRHRH1dXd1+HciDaGZmpdXkGHsjML1gfVoqK1Vng6QaYDzQ0s++/cUsVg8sTPdTpgDvk9QZEb8Y8JmYmdnLkmfPZjkwS9JMSbVkN/wbiuo0AOel5XOAOyMiUvm8NFttJjALWDbAmL1ExMyImBERM8juC30u70TjB3GamfWWW7JJ92AuApYAjwGLImKlpMslnZ2qXQdMltQEXAxcmvZdCSwCVgF3ABdGRFe5mACSPi9pA1lvZ4WkH+R1buV4MpqZWWl5DqMREYuBxUVllxUstwIfKbPvAmDBQGKm8u8C3+2nPZ8aSLvNzKyyDoYJAmZmNsScbHLgqc9mZr052VSQ79mYmZXmZGNmZrlzssmBR9HMzHpzsqkg+RkCZmYlOdmYmVnunGxyEJ6OZmbWi5NNJXkUzcysJCcbMzPLnZNNDjyIZmbWm5NNBXkUzcysNCcbMzPLnZONmZnlzskmB575bGbWm5NNBclP4jQzKynXZCNpjqTVkpokXVpi+0hJN6ftSyXNKNg2P5WvlnRmfzElXZTKQtKUgvKPSVoh6U+S/ijpxBxP2czMSsgt2UiqBq4CzgJmA+dKml1U7Xxga0QcB1wJXJH2nQ3MA04A5gBXS6ruJ+a9wOnAU0XHWAu8KyJeD/wTcG1FT7Qkj6OZmRXKs2dzMtAUEWsioh1YCMwtqjMXuCEt3wKcpmwsai6wMCLaImIt0JTilY0ZEQ9FxLriRkTEHyNia1q9H5hWyZMs5EE0M7PS8kw2U4H1BesbUlnJOhHRCWwHJvex70Bi9uV84NelNki6QFKjpMbm5uZ9CGlmZv05aCYISPpLsmRzSantEXFtRNRHRH1dXd3LOpZno5mZ9VaTY+yNwPSC9WmprFSdDZJqgPFASz/79hdzL5LeAPwAOCsiWvbhHPaJJ6OZmZWWZ89mOTBL0kxJtWQ3/BuK6jQA56Xlc4A7I3s+fwMwL81WmwnMApYNMGYvko4GbgU+ERGPV+jczMxsH+TWs4mITkkXAUuAauD6iFgp6XKgMSIagOuAGyU1AVvIkgep3iJgFdAJXBgRXZBNcS6Omco/D3wZOAJYIWlxRHwauIzsPtDV6XswnRFRn9d5g+eimZkVk1/0tbf6+vpobGzc5/3ueWIzH79uKT/77Fv5ixmTcmiZmdnwJemBcn/MHzQTBMzMbOg42ZiZWe6cbHLgkUkzs96cbCrIU5/NzEpzsjEzs9w52eTAM/zMzHpzsqkgj6KZmZXmZGNmZrlzssmBB9HMzHpzsqkkj6OZmZXkZGNmZrlzssmBJ6OZmfXmZFNB8jiamVlJTjZmZpY7JxszM8tdrslG0hxJqyU1Sbq0xPaRkm5O25dKmlGwbX4qXy3pzP5iSroolYWkKQXlkvTdtG2FpDfleMoAhCc/m5n1kluykVQNXAWcBcwGzpU0u6ja+cDWiDgOuBK4Iu07m+ytnScAc8jeslndT8x7gdOBp4qOcRbZa6VnARcA11TyPAv5QZxmZqXl2bM5GWiKiDUR0Q4sBOYW1ZkL3JCWbwFOU/bu5rnAwohoi4i1QFOKVzZmRDwUEetKtGMu8KPI3A9MkHRkRc/UzMz6lGeymQqsL1jfkMpK1omITmA7MLmPfQcSc3/aUVkeRTMz68UTBBJJF0hqlNTY3Ny8fzEq3CYzs1eKPJPNRmB6wfq0VFayjqQaYDzQ0se+A4m5P+0gIq6NiPqIqK+rq+snpJmZ7Ys8k81yYJakmZJqyW74NxTVaQDOS8vnAHdG9jKYBmBemq02k+zm/rIBxizWAHwyzUo7BdgeEc9W4gTL8SiamVlvNXkFjohOSRcBS4Bq4PqIWCnpcqAxIhqA64AbJTUBW8iSB6neImAV0AlcGBFdkE1xLo6Zyj8PfBk4AlghaXFEfBpYDLyPbJLBbuBv8zpneTqamVlJuSUbgIhYTPZhX1h2WcFyK/CRMvsuABYMJGYq/y7w3RLlAVy4r203M7PK8QSBHPhBnGZmvTnZVJBH0czMSnOyMTOz3DnZmJlZ7pxscuAHcZqZ9eZkU0G+ZWNmVpqTjZmZ5c7JJgee+mxm1puTTQV56rOZWWlONmZmljsnmxx4FM3MrDcnm4ryOJqZWSlONmZmlrsBJRtJX5A0Lr0T5jpJD0o6I+/GHajC09HMzHoZaM/mv0fEC8AZwETgE8A3c2uVmZm9ogw02fTcjHgfcGN6YZlvUBTx1Gczs9IGmmwekPQbsmSzRNJYoLu/nSTNkbRaUpOkS0tsHynp5rR9qaQZBdvmp/LVks7sL2Z6VfTSVH5zem00ko6WdJekhyStkPS+AZ6zmZlVyECTzfnApcBfRMRuYAT9vF5ZUjVwFXAWMBs4V9LsEnG3RsRxwJXAFWnf2WSviD4BmANcLam6n5hXAFemWFtTbICvAosi4o0p5tUDPOf91nPH5rLbH+U1//jrvA9nZjbsDTTZvBVYHRHbJH2c7AN8ez/7nAw0RcSaiGgHFgJzi+rMBW5Iy7cAp0lSKl8YEW0RsRZoSvFKxkz7vCfFIMX8YFoOYFxaHg88M8Bz3mfFo2g/uu8pWjv67QCamb3iDTTZXAPslnQi8CXgSeBH/ewzFVhfsL4hlZWsExGdZAlsch/7liufDGxLMYqP9XXg45I2AIuBvy/VWEkXSGqU1Njc3NzPqZmZ2b4YaLLpjGw+71zg/4uIq4Cx+TWros4F/k9ETCNNcJC013lHxLURUR8R9XV1dS/viJ75bGbWy0CTzQ5J88mmPP8qfViP6GefjcD0gvVpqaxkHUk1ZMNcLX3sW668BZiQYhQf63xgEUBE3AeMAqb00/b9Ik9HMzMraaDJ5qNAG9n3bZ4j+zD/dj/7LAdmpVlitWQ35xuK6jQA56Xlc4A7Uw+qAZiXZqvNBGYBy8rFTPvclWKQYt6elp8GTgOQ9FqyZONxMjOzQTSgZJMSzE3AeEkfAFojos97Nun+yUXAEuAxshlhKyVdLunsVO06YLKkJuBishlvpO/xLAJWAXcAF0ZEV7mYKdYlwMUp1uQUG7J7TJ+R9AjwU+BTkfNX/P1aaDOz3mr6rwKS/pqsJ3M32aSrf5f0DxFxS1/7RcRispvyhWWXFSy3Ah8ps+8CYMFAYqbyNWSz1YrLVwFv76udleJBNDOz0gaUbICvkH3HZhOApDrgt7w01djMzKysgd6zqepJNEnLPuxrZmYHuYH2bO6QtITsngdkEwb2GsqyjB/6bGbW24CSTUT8g6QP89K9j2sj4rb8mnVg8sxnM7PSBtqzISJ+Dvw8x7aYmdkrVJ/JRtIOSn8fXkBExLgS2w56HkYzM+utz2QTEQfKI2mGBXnys5lZSZ5RZmZmuXOyyYFH0czMenOyqSDPRjMzK83JxszMcudkk4Ocn/NpZnbAcbIxM7PcOdmYmVnunGzMzCx3TjY58B0bM7Peck02kuZIWi2pSdKlJbaPlHRz2r5U0oyCbfNT+WpJZ/YXM70qemkqvzm9Nrpn219LWiVppaSf5He+eUU2Mzuw5ZZsJFUDVwFnAbOBcyXNLqp2PrA1Io4DrgSuSPvOBuYBJwBzgKslVfcT8wrgyhRra4qNpFnAfODtEXEC8MV8zri3h9dvG4zDmJkdEPLs2ZwMNEXEmohoBxYCc4vqzAVuSMu3AKdJUipfGBFtEbEWaErxSsZM+7yHl94cegPwwbT8GeCqiNgKUPQSuFxEwMNPb837MGZmB4w8k81UYH3B+oZUVrJORHQC24HJfexbrnwysC3FKD7W8cDxku6VdL+kOaUaK+kCSY2SGpubm/fpRPfE8IM4zcxKOhgmCNQAs4B3A+cC/yFpQnGliLg2Iuojor6urm5wW2hm9gqXZ7LZCEwvWJ+WykrWkVQDjAda+ti3XHkLMCHFKD7WBqAhIjrSkNzjZMknR3vPR9u0o5WdbZ0l6pqZvfLlmWyWA7PSLLFashv+DUV1GoDz0vI5wJ2RPeulAZiXZqvNJEsOy8rFTPvclWKQYt6eln9B1qtB0hSyYbU1FT5Xsvjlt5284Hec9Z0/5HFYM7Nhb8Cvhd5XEdEp6SJgCVANXB8RKyVdDjRGRANwHXCjpCZgC1nyINVbBKwCOoELI6ILoFTMdMhLgIWSvgE8lGKT6p4haRXQBfxDRLTkdd59Wb/lxaE4rJnZkMst2QBExGJgcVHZZQXLrcBHyuy7AFgwkJipfA3ZbLXi8gAuTj+Dws/hNDPr7WCYIDBo/KVOM7PSnGzMzCx3TjZmZpY7J5scFN+y8cvUzOxg52RTQeWeIOBcY2YHOycbMzPLnZNNDop7Mu7YmNnBzsmmgspNffY9GzM72DnZmJlZ7pxschBFA2fu15jZwc7JpoLKPUDAo2hmdrBzsjEzs9w52eRg79lo7tqY2cHNyaaC/CBOM7PSnGwGge/ZmNnBLtdkI2mOpNWSmiRdWmL7SEk3p+1LJc0o2DY/la+WdGZ/MdPbO5em8pvTmzwLj/VhSSGpPqfTNTOzMnJLNpKqgauAs4DZwLmSZhdVOx/YGhHHAVcCV6R9Z5O9tfMEYA5wtaTqfmJeAVyZYm1NsXvaMhb4ArA0j3Mt5o6MmVlvefZsTgaaImJNRLQDC4G5RXXmAjek5VuA0yQplS+MiLaIWAs0pXglY6Z93pNikGJ+sOA4/0SWjForfI5F/CBOM7NS8kw2U4H1BesbUlnJOhHRCWwHJvexb7nyycC2FKPXsSS9CZgeEb96+ae0fzwbzcwOdq/oCQKSqoB/Bb40gLoXSGqU1Njc3PyyjutnoZmZ9ZZnstkITC9Yn5bKStaRVAOMB1r62LdceQswIcUoLB8LvA64W9I64BSgodQkgYi4NiLqI6K+rq5un082O4fS5c49ZnawyzPZLAdmpVlitWQ3/BuK6jQA56Xlc4A7I+sWNADz0my1mcAsYFm5mGmfu1IMUszbI2J7REyJiBkRMQO4Hzg7IhrzOulSnGvM7GBX03+V/RMRnZIuApYA1cD1EbFS0uVAY0Q0ANcBN0pqAraQJQ9SvUXAKqATuDAiugBKxUyHvARYKOkbwEMptpmZDQO5JRuAiFgMLC4qu6xguRX4SJl9FwALBhIzla8hm63WV3vePZB276/yD+J038bMDm6v6AkCw4VTjZkd7JxsclDckbnnic1D0xAzs2HCyaaCVGY62uduenCQW2JmNrw42ZiZWe6cbMzMLHdONjnw42nMzHpzsqkgvzvNzKw0JxszM8udk00O/B1OM7PenGwqqNyDOM3MDnZONmZmljsnmxx4GM3MrDcnmwqS56OZmZXkZGNmZrlzsslBAN+8489D3Qwzs2HDyaaCCmejtXZ0D11DzMyGmVyTjaQ5klZLapJ0aYntIyXdnLYvlTSjYNv8VL5a0pn9xUyvil6aym9Or41G0sWSVklaIel3ko7J85zNzGxvuSUbSdXAVcBZwGzgXEmzi6qdD2yNiOOAK4Er0r6zyV4RfQIwB7haUnU/Ma8ArkyxtqbYkL0iuj4i3gDcAnwrj/M1M7Py8uzZnAw0RcSaiGgHFgJzi+rMBW5Iy7cApyl7KcxcYGFEtEXEWqApxSsZM+3znhSDFPODABFxV0TsTuX3A9Mqf6q9dXvus5lZL3kmm6nA+oL1DamsZJ2I6AS2A5P72Ldc+WRgW4pR7liQ9XZ+Xaqxki6Q1Cipsbm5ud+TK6WqKt20ca4xM+vloJkgIOnjQD3w7VLbI+LaiKiPiPq6urr9OkZPrulyz8bMrJeaHGNvBKYXrE9LZaXqbJBUA4wHWvrZt1R5CzBBUk3q3fQ6lqTTga8A74qItpd5XmVVp+loHkYzM+stz57NcmBWmiVWS3bDv6GoTgNwXlo+B7gzIiKVz0uz1WYCs4Bl5WKmfe5KMUgxbweQ9Ebg+8DZEbEpp3MlHQuAbucaM7NecuvZRESnpIuAJUA1cH1ErJR0OdAYEQ3AdcCNkpqALWTJg1RvEbAK6AQujIgugFIx0yEvARZK+gbZDLTrUvm3gUOBn6Vk8HREnJ3HOe+5ZeOejZlZL3kOoxERi4HFRWWXFSy3Ah8ps+8CYMFAYqbyNWSz1YrLT9/nhu+nqp6ejbs2Zma9HDQTBAZDlYfRzMxKcrKpIKWr6QkCZma9OdlUkGejmZmV5mRTQR5GMzMrzcmmgnqe+uyejZlZb042FdTTs+kr1+xu76TLXR8zO8g42VRQz/ds+pr6PPuyJXyt4dFBapGZ2fDgZFNBPT2b/p6Ntmj5Bh54agv3PdnSq7yts4tP/XAZq555Ibc2mg3E7Q9vZMalv2JXW2f/lc0GwMmmgl66Z1N6+5RDR+5Z/vA193Huf9zfa/uqZ17g7tXNzL/tT3k10WxAvvO7JwB4dvuLvcq37+4YiubYK4CTTQVJokrlH1czurZ6r7Ib738q72aZvQwvvev8gae2cOLlv2HJyueGsD12oHKyqbAqqexstM6u7r3K/vEXj+41VLGjtYMLf/IgP132NB0l9jHLXYl/wo+s3w6w1/Cv2UDk+my0g1GWbEpv6yizobOofE3zLtY07+JXK55l0wttfOH0WZVuptkADd7Mya272nl4/Tbe/eq6PU9Qt1cO92wqTCo/G61cL6Wv3svGbbvLbjPLS8+/4I6uwUs2P7x3LX/7f5bz28dyfROIDREnmwqrkvbqqfToLPOL296ZJRt/+waebN7JjEt/xb1NmwFo7ejiz8/lPzuvo6u7z+8/nXnlH/jRfete1jE+86NG3vuvv39ZMQZbuX+zediRhpPn37qCm5Y+xZrmnR5GfgVxsqmwKpW+NwMv9WDai7b3lJf6xRYH13DC0jVbAGh4+BkA/ufPHmHOv/0XL7TmOwtq1ld+3Wt24KLl67n2D08C2YSP1c/v4LLbV5bbfUD+c9XzPLFp5571H/zXGh5ev63f/e5evYmv/mJwZyj2THLp6H7p3+qCxY/12v7oxu0VPWZPsp82cTRfue1R3vP//p7X/OMdnPqtu/jsjQ/wx6bNtHZ0VfSYNnh8z6bCqqrK92zaOksnoZ6eTakkdbAMXUcEKzbs/eF1/5rsZvTz21vZsrOdGVPG7FXnVyue5S9fU8fo2pf3z3nZ2izRfWnRI/z8wQ0AfOadx7LgV4+VrL+zrZOfP7CBT5xyDFVVff+PKp5CDPCNFPfnf/c23nzMxLL7fuqHywH4p7mv2+d7GU+37KaqKvsAh+y7XP+yZDUX/uVxTBhd2+/+hX8AFfb8fnTfU3ytYSULLziFqRMO4ZltL/KWYyeXjPHDe9fy2iPHccqxk9nR2sGSlc/z4TdN3etcWju6OGLcKG773NtYsWE7T2zaybrNu1jXsov7nmzhjpXPIcHRk0Zz9KTRTJ1wCNMnjWb6pNFMnTCK4w4by7hRNb7fM0zlmmwkzQG+Q/ZWzR9ExDeLto8EfgS8GWgBPhoR69K2+cD5QBfw+YhY0lfM9ProhcBk4AHgExHR3tcx8tDXbLRy2ru62dHawWd//MBe2w6W35ufP7iR//mzR/ast+xqZ/m6LWxN3+t4/3fvob2rm3XffP+eOnc8+ixd3XDhTx7ko/XTueKcN/R5jB/f/xRvmDae1x01npZd7fzVNffyH5+s32v4rCfRAFx/7zp+cM/aPeszLv0Vqy4/k3+/s4lr7s56Pl9rWMm5Jx/NP//V68se+63/fGev9bbOl/5C//A1f6RpwVlIorqPpLVs7RZ2d3Sx+rkdnH3iUbzjijs5bOwoqgRbdrfT2tHNgg+9jo+95Zg9+5z67bsA+PM/zWHUiGruePQ5/uO/1rK7vYsFHyrf3h6tHV1c/n9X8eDTW3uVL1+XJeaNW19k3rVZj3DdN9/PzrZOaqurqK3JBk0igv/9f1ft2f73P32Iu1c3M+XQLNHVjR1J3diRPLe9lUWNGxg7MksWJ06fwInTJ+w53ovtXdy1ehOPbtzOupZsAs2DT21lV3vvns4hI6qZdfihHD1pNDOnjGH6pNGMGzWCsaNqOKS2mnGjRjB5TC0TRo/Yr6S0ZVc7Xd3B2FE1bN7ZtieJQ/Y1hmMmjebU4+v2Oe7BQHm9wlhSNfA48F5gA7AcODciVhXU+Rzwhoj4rKR5wIci4qOSZgM/JXvz5lHAb4Hj024lY6bXSN8aEQslfQ94JCKuKXeMvtpeX18fjY2N+3Xer/7qr4nYe6isL59+x0we2bCN5eu2lq3zrXPewGmvOYzbHtrI7Q8/w8VnHM87jptCdwQja176/k5E0NrRTW1NFauf28FRE0axbXcHz2x7ka27O+iO4LCxIxkzsoZj68bQ2tFNlbIvorZ3djNxzAgaHn6GnW2drH5uB18649WMHVVDBDyz/UXueWIzNdVi5uQxvPVVk3n8+Z0EwXF1h/L8jjbWbd7FrQ9u5C0zJ/HmGRN5/oVW/uFnK/ji6bOYOvEQZh85jtG1NazYsI0Jo2t5Vd0YJPG/bvsTP1n6dL/X6r++/Jc072yjtrqKD/z7Pb22/T/vOpZXTTmUs086ipqq7IP7yeZdTBg9glO/dRe7Cz6Y/u7dr+Kau5/k5JmT9vRoAP75r17P/FsrM2Q1+8hxtHV2ccqxk7mp4NyOP/xQPn/aLC76yUN7yj5+ytH8+P6n+fCbptGyq417mzbztf92Avc2bebXj+7b91ru/NK7OGbyGKqrxIxLf7Wn/DvzTuILCx8GoLa6igcvey+jR1Tz0Ppt3PLAek45djJzT5rKrQ9u4OJFWeL/7Ltexfd+/2Sv+K85YizPvdDKtt0dnHLsJO5PQ5891xSy5AZZ4v7Kbdnjmb76/tfu6c31pfAPir5EBLvau9i49UWeatnFms27eG57K0827+Tx53fw/Attfe5/6vF1/M3JRzNt4iEcNnYkkw8duVey37a7nd8+tgkB9TMm8q5v371XnNccMZY/P7djz/r3P/Fm6saO5E1Hl++t9nVOhUmweD0P23a3M/6Q/Uu+xSQ9EBH1JbflmGzeCnw9Is5M6/MBIuKfC+osSXXuk1QDPAfUAZcW1u2pl3bbKybwTaAZOCIiOguPXe4Y0ceJv5xk89/+/R7+tJ9j2TOnjGHt5l37vF91lZg4egTdkf3lNVhqa6r2DAG+HJPH1NJS4XbveU7dQTrrYkS1mDxmJM+90LpP+00aUzuo/4aKTRg9gocvO6MisTbvbGPj1hfZ1d7J8y+0snVXBzvbOmnt6GLLrnb+c9Xzvf7dVQkmjRlJbbWorhYdnbHP16/Q4eNGMqK6igioqRadXUF7mojS1R1EBKNGVBNkM1jbO7vZ0dbJYWNH0trRxcgR1Wzf3cEhtdXUVIlRI6rp6OpmR2snXRGIbFKRyEZAIrL/1lRVMbIm612OGpH9IVoqj4hstuHTW7IZr0eNH8Wo2mr+5uSj+fQ7j92vc+4r2eQ5jDYVWF+wvgF4S7k6KUlsJxsGmwrcX7Tv1LRcKuZkYFtEdJaoX+4YmwsbIukC4AKAo48+el/Os5frzqtn0442Dqmt5slNO3n7cVO45OcreM9rDuO4ww7l2e2t3Nu0mR/d9xSvnzqeT771GDbtaOPNx0zkLTMncf2963h043aebN7JSdMn0LKrnZoqcczkMUQEG7a+yGPPvkB3BKe/9nC6uoOdbZ0EPZMTgpZd7UyfOJpl61qYOLqWpWu2MHZUDSNrqnj2hVbe8+rD2LyrnVmHHcrWXe2s2byL4w47lDXNOzl60mjuWt3MCUeNo72zm9lHjWPi6FpeaO3Yc/O+ukrsbu/ifa8/gu0vdjBmZA211VWsa8mGNsaPHsGuti6mTxpNR2c3T2zawRmzjyAIjp40ht3tnSxcvp5TZ03hiPGjiMiGZQ4bO4oZU0ZzxuwjuPYPa5gxZTS/XPEsXd3BjtZODh1Zw862Tt549ATGHzKCYyaN5v1vOIp7nmjmnqbNHDnhEHa0dnLslDGMG1XD9hc7+OOTLZxxwuEsXLaecYeMoLWjizdMG8/E0bWsevYFZh02lj9t3MakMbW8fup4ntneyrhRI5g0ZgRHjD+EXz7yDGNH1fDn53aws62Tz7zzWB56eitrmnfx6iPGcvi4UTTvaKOzu5vjDx/L2Scexd2rm5kxZQyPP7+Dts5uGtdtoX7GJFo7unjX8XWsa9nFivXb2bK7naZNO9mwdTdvmTmZt8ycxD1Nm2nr7GbzzjYi4G2vmszu9i6Wrt1CZ3c3H3rjVP787A5e7OjipOkT6OoO1m7exagRVRw5/hCC4A+Pb2bmlDEcMW4Ua1t2MW7UCKZOGMWLHV10dcPUCaPY1d7F/WtaOHzcKDZufZE3Hj2BCaNr2dHaQVtnNx1d3RwyopqaanHPE5upnzGJk2dO4vVTx3PxokfY0drBX9dPZ/2W3VlPd8oYHn9+JyOqq+js6mbMyBo6u7tp2rSTE6dNYP3W3VkPZMtupk8czQlHjeOXK57l1OOnMOXQkTyyfhtvO24Kn39P5b5TNuXQkb0eEVXs62d3serZF9j0QhvNO1rZtKONzTvb6OgKuruD1s4unty0i9XP7+i1X89IQOFx3jv7MJo27aR5RxvrWnbzzllTqK2uYvwhI4Dsu3Q11WJkTRXVVaJaQhKtHV1Ioq2ji47u2NPLqK2uoqZaexJVd3c2kag7jVwEgRBB7HnKfM9nQM/v6IjqKrojSyo9yalH4d9gT2/ZzWmvOYzxh4ygrbOburHlr9nLkWfP5hxgTkR8Oq1/AnhLRFxUUOfRVGdDWn+SLHl8Hbg/In6cyq8Dfp122ytmQf3jUvl04NcR8bpyx4iIXsmm0Mvp2ZiZHaz66tnkOfV5IzC9YH1aKitZJw1xjSe7iV9u33LlLcCEFKP4WOWOYWZmgyTPZLMcmCVppqRaYB7QUFSnATgvLZ8D3JnupTQA8ySNTLPMZgHLysVM+9yVYpBi3t7PMczMbJDkds8m3R+5CFhCNk35+ohYKelyoDEiGoDrgBslNQFbyJIHqd4iYBXQCVwYEV0ApWKmQ14CLJT0DeChFJtyxzAzs8GT2z2bA5nv2ZiZ7buhumdjZmYGONmYmdkgcLIxM7PcOdmYmVnuPEGgBEnNwFP7ufsUip5OMEwdCO08ENoIbmclHQhthAOjnUPRxmMiouSTSJ1sKkxSY7nZGMPJgdDOA6GN4HZW0oHQRjgw2jnc2uhhNDMzy52TjZmZ5c7JpvKuHeoGDNCB0M4DoY3gdlbSgdBGODDaOaza6Hs2ZmaWO/dszMwsd042ZmaWOyebCpI0R9JqSU2SLh3CdkyXdJekVZJWSvpCKp8k6T8lPZH+OzGVS9J3U7tXSHrTILa1WtJDkn6Z1mdKWpracnN6lQTpdRM3p/KlkmYMYhsnSLpF0p8lPSbprcP0Wv6P9P/7UUk/lTRqOFxPSddL2pReZNhTts/XT9J5qf4Tks4rdawKt/Hb6f/5Ckm3SZpQsG1+auNqSWcWlOf6GVCqnQXbviQpJE1J60NyLcuKCP9U4IfslQdPAscCtcAjwOwhasuRwJvS8ljgcWA28C3g0lR+KXBFWn4f2ZtQBZwCLB3Etl4M/AT4ZVpfBMxLy98D/i4tfw74XlqeB9w8iG28Afh0Wq4FJgy3a0n2+vO1wCEF1/FTw+F6AqcCbwIeLSjbp+sHTALWpP9OTMsTc27jGUBNWr6ioI2z0+/3SGBm+r2vHozPgFLtTOXTyV698hQwZSivZdm2D8YvwsHwA7wVWFKwPh+YP9TtSm25HXgvsBo4MpUdCaxOy98Hzi2ov6dezu2aBvwOeA/wy/RLsbngF3zPNU2/SG9NyzWpngahjePTh7iKyofbtZwKrE8fIDXpep45XK4nMKPog3yfrh9wLvD9gvJe9fJoY9G2DwE3peVev9s913KwPgNKtRO4BTgRWMdLyWbIrmWpHw+jVU7PL3uPDalsSKXhkTcCS4HDI+LZtOk54PC0PFRt/zfgy0B3Wp8MbIuIzhLt2NPGtH17qp+3mUAz8MM03PcDSWMYZtcyIjYC/wI8DTxLdn0eYPhdzx77ev2G+vfrv5P1EuijLUPSRklzgY0R8UjRpmHVTiebVzBJhwI/B74YES8UbovsT5ohm/cu6QPApoh4YKjaMEA1ZMMW10TEG4FdZMM+ewz1tQRI9zzmkiXHo4AxwJyhbNNADYfr1xdJXyF7Y/BNQ92WYpJGA/8LuGyo29IfJ5vK2Ug2btpjWiobEpJGkCWamyLi1lT8vKQj0/YjgU2pfCja/nbgbEnrgIVkQ2nfASZI6nldeWE79rQxbR8PtOTcRsj+6tsQEUvT+i1kyWc4XUuA04G1EdEcER3ArWTXeLhdzx77ev2G5LpK+hTwAeBjKSkOtza+iuwPjEfS79I04EFJRwyzdjrZVNByYFaa/VNLdtO1YSgaIknAdcBjEfGvBZsagJ6ZJ+eR3cvpKf9kmr1yCrC9YIgjFxExPyKmRcQMsmt1Z0R8DLgLOKdMG3vafk6qn/tfwxHxHLBe0qtT0WnAKobRtUyeBk6RNDr9/+9p57C6ngX29fotAc6QNDH14s5IZbmRNIdsmPfsiNhd1PZ5aUbfTGAWsIwh+AyIiD9FxGERMSP9Lm0gmxz0HMPoWvY01j+Vu3H3PrKZX08CXxnCdryDbFhiBfBw+nkf2Zj874AngN8Ck1J9AVeldv8JqB/k9r6bl2ajHUv2i9sE/AwYmcpHpfWmtP3YQWzfSUBjup6/IJvBM+yuJfC/gT8DjwI3ks2WGvLrCfyU7D5SB9mH4fn7c/3I7ps0pZ+/HYQ2NpHd2+j5HfpeQf2vpDauBs4qKM/1M6BUO4u2r+OlCQJDci3L/fhxNWZmljsPo5mZWe6cbMzMLHdONmZmljsnGzMzy52TjZmZ5c7JxuwVQNK7lZ6cbTYcOdmYmVnunGzMBpGkj0taJulhSd9X9j6fnZKuVPYumt9Jqkt1T5J0f8H7VHre+XKcpN9KekTSg5JelcIfqpfeu3NTepIAkr6p7N1GKyT9yxCduh3knGzMBomk1wIfBd4eEScBXcDHyB6a2RgRJwC/B76WdvkRcElEvIHsG+A95TcBV0XEicDbyL5RDtnTvb9I9r6VY4G3S5pM9nj8E1Kcb+R5jmblONmYDZ7TgDcDyyU9nNaPJXvFws2pzo+Bd0gaD0yIiN+n8huAUyWNBaZGxG0AEdEaLz23a1lEbIiIbrLHq8wge3VAK3CdpL8CCp/xZTZonGzMBo+AGyLipPTz6oj4eol6+/sMqbaC5S6yl6Z1AieTPa36A8Ad+xnb7GVxsjEbPL8DzpF0GICkSZKOIfs97Hky898A90TEdmCrpHem8k8Av4+IHcAGSR9MMUamd5qUlN5pND4iFgP/g+xtjmaDrqb/KmZWCRGxStJXgd9IqiJ7cu+FZC9kOzlt20R2XweyR+9/LyWTNcDfpvJPAN+XdHmK8ZE+DjsWuF3SKLKe1cUVPi2zAfFTn82GmKSdEXHoULfDLE8eRjMzs9y5Z2NmZrlzz8bMzHLnZGNmZrlzsjEzs9w52ZiZWe6cbMzMLHf/P4Azy4WT7atFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot loss function\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(loss_epoch, loss_values)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.semilogy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbfec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cpu')\n",
    "model=model.to(device)\n",
    "PATH= \"model_jit_E.pth\"\n",
    "traced_net=torch.jit.trace(model, (torch.randn(1,6400).to(torch.float64)).to(device))\n",
    "traced_net.to(torch.float64)\n",
    "torch.jit.save(traced_net, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e6e2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
